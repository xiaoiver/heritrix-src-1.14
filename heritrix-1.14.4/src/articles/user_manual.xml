<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">
<article>
  <title>Heritrix User Manual</title>

  <articleinfo>
    <date>$Date: 2007-11-15 22:35:26 +0000 (Thu, 15 Nov 2007) $</date>

    <authorgroup>
      <corpauthor>Internet Archive</corpauthor>

      <author>
        <firstname>Kristinn</firstname>

        <surname>SigurÄ‘sson</surname>
      </author>

      <author>
        <firstname>Michael</firstname>

        <surname>Stack</surname>
      </author>

      <author>
        <firstname>Igor</firstname>

        <surname>Ranitovic</surname>
      </author>
    </authorgroup>
  </articleinfo>

  <sect1 id="intro">
    <title>Introduction</title>

    <para>Heritrix is the Internet Archive's open-source, extensible,
    web-scale, archival-quality web crawler.</para>

    <para>This document explains how to create, configure and run crawls using
    Heritrix. It is intended for users of the software and presumes that they
    possess at least a general familiarity with the concept of web
    crawling.</para>

    <para>For a general overview on Heritrix, see <ulink
    url="/An Introduction to Heritrix.pdf">An Introduction to
    Heritrix</ulink>.</para>

    <para>If you want to build Heritrix from source or if you'd like to make
    contributions and would like to know about contribution conventions, etc.,
    see instead the <ulink
    url="http://crawler.archive.org/articles/developer_manual/index.html">Developer's
    Manual</ulink>.</para>
  </sect1>

  <sect1 id="install">
    <title>Installing and running Heritrix</title>

    <para>This chapter will explain how to set up Heritrix.</para>

    <para>Because Heritrix is a pure Java program it can (in theory anyway) be
    run on any platform that has a Java 5.0 VM. However we are only committed
    to supporting its operation on Linux and so this chapter only covers setup
    on that platform. Because of this, what follows assumes basic Linux
    administration skills. Other chapters in the user manual are platform
    agnostic.</para>

    <para>This chapter also only covers installing and running the prepackaged
    binary distributions of Heritrix. For information about downloading and
    compiling the source see the <ulink
    url="http://crawler.archive.org/articles/developer_manual/index.html">Developer's
    Manual</ulink>.</para>

    <sect2>
      <title>Obtaining and installing Heritrix</title>

      <para>The packaged binary can be downloaded from the project's <ulink
      url="http://sourceforge.net/projects/archive-crawler">sourceforge home
      page</ulink>. Each release comes in four flavors, packaged as .tar.gz or
      .zip and including source or not.</para>

      <para>For installation on Linux get the file
      <filename>heritrix-?.?.?.tar.gz</filename> (where ?.?.? is the most
      recent version number).</para>

      <para>The packaged binary comes largely ready to run. Once downloaded it
      can be untarred into the desired directory.</para>

      <para><programlisting>  % tar xfz heritrix-?.?.?.tar.gz</programlisting></para>

      <para>Once you have downloaded and untarred the correct file you can
      move on to the next step.</para>

      <sect3>
        <title>System requirements</title>

        <sect4>
          <title>Java Runtime Environment</title>

          <para>The Heritrix crawler is implemented purely in Java. This means
          that the only true requirement for running it is that you have a JRE
          installed (Building will require a JDK).</para>

          <para>The Heritrix crawler, since release 1.10.0, makes use of Java
          5.0 features so your JRE must be at least of a 5.0 (1.5.0+)
          pedigree.</para>

          <para>We currently include all of the free/open source third-party
          libraries necessary to run Heritrix in the distribution package. See
          <ulink
          url="http://crawler.archive.org/dependencies.html">dependencies</ulink>
          for the complete list (Licenses for all of the listed libraries are
          listed in the dependencies section of the raw project.xml at the
          root of the <literal>src</literal> download or on
          Sourceforge).</para>

          <sect5>
            <title>Installing Java</title>

            <para>If you do not have Java installed you can download Java
            from:</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">Sun</emphasis> -- <ulink
                url="http://java.sun.com/">java.sun.com</ulink></para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">IBM</emphasis> -- <ulink
                url="http://www.ibm.com/java">www.ibm.com/java</ulink></para>
              </listitem>
            </itemizedlist>
          </sect5>
        </sect4>

        <sect4>
          <title>Hardware</title>

          <para>A default java heap is 256MB RAM, which is usually suitable
          for crawls that range over hundreds of hosts. Assign more -- see
          <xref linkend="java_opts" /> for how -- of your available RAM to the
          heap if you are crawling thousands of hosts or experience Java
          out-of-memory problems.</para>
        </sect4>

        <sect4>
          <title>Linux</title>

          <para>The Heritrix crawler has been built and tested primarily on
          Linux. It has seen some informal use on Macintosh, Windows 2000 and
          Windows XP, but is not tested, packaged, nor supported on platforms
          other than Linux at this time.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2>
      <title>Running Heritrix</title>

      <para>To run Heritrix, first do the following: <programlisting>  % export HERITRIX_HOME=/PATH/TO/BUILT/HERITRIX</programlisting>...where
      <literal>$HERITRIX_HOME</literal> is the location of your untarred
      <filename>heritrix.?.?.?.tar.gz</filename>.</para>

      <para>Next run:<programlisting>  % cd $HERITRIX_HOME
  % chmod u+x $HERITRIX_HOME/bin/heritrix
  % $HERITRIX_HOME/bin/heritrix --help</programlisting>This should give you
      usage output like the following:</para>

      <para><programlisting><computeroutput>  Usage: heritrix --help
  Usage: heritrix --nowui ORDER.XML
  Usage: heritrix [--port=#] [--run] [--bind=IP,IP...] --admin=LOGIN:PASSWORD \
      [ORDER.XML]
  Usage: heritrix [--port=#] --selftest[=TESTNAME]
  Version: @VERSION@
  Options:
   -b,--bind       Comma-separated list of IP addresses or hostnames for web
                   server to listen on.  Set to / to listen on all available
                   network interfaces.  Default is 127.0.0.1.
   -a,--admin      Login and password for web user interface administration.
                   Required (unless passed via the 'heritrix.cmdline.admin'
                   system property).  Pass value of the form 'LOGIN:PASSWORD'.
   -h,--help       Prints this message and exits.
   -n,--nowui      Put heritrix into run mode and begin crawl using ORDER.XML. Do
                   not put up web user interface.
   -p,--port       Port to run web user interface on.  Default: 8080.
   -r,--run        Put heritrix into run mode. If ORDER.XML begin crawl.
   -s,--selftest   Run the integrated selftests. Pass test name to test it only
                   (Case sensitive: E.g. pass 'Charset' to run charset selftest).
  Arguments:
   ORDER.XML       Crawl order to run.</computeroutput></programlisting>Launch
      the crawler with the UI enabled by doing the following:</para>

      <para><programlisting>  % $HERITRIX_HOME/bin/heritrix --admin=LOGIN:PASSWORD</programlisting>This
      will start up Heritrix printing out a startup message that looks like
      the following:</para>

      <para><programlisting>  [b116-dyn-60 619] heritrix-0.4.0 &gt; ./bin/heritrix
  Tue Feb 10 17:03:01 PST 2004 Starting heritrix...
  Tue Feb 10 17:03:05 PST 2004 Heritrix 0.4.0 is running.
  Web UI is at: http://b116-dyn-60.archive.org:8080/admin
  Login and password: admin/letmein</programlisting> <note>
          <para>By default, as of version 1.10.x, Heritrix binds to localhost
          only. This means that you need to be running Heritrix on the same
          machine as your browser to access the Heritrix UI. Read about the
          <literal>--bind</literal> argument above if you need to access the
          Heritrix UI over a network.</para>
        </note></para>

      <para>See <xref linkend="wui" /> and <xref linkend="tutorial" /> to get
      your first crawl up and running.</para>

      <sect3>
        <title>Environment variables</title>

        <para>Below are environment variables that effect Heritrix
        operation.</para>

        <sect4>
          <title>HERITRIX_HOME</title>

          <para>Set this environment variable to point at the Heritrix home
          directory. For example, if you've unpacked Heritrix in your home
          directory and Heritrix is sitting in the heritrix-1.0.0 directory,
          you'd set HERITRIX_HOME as follows. Assuming your shell is
          bash:<programlisting>  % export HERITRIX_HOME=~/heritrix-1.0.0</programlisting>If
          you don't set this environment variable, the Heritrix start script
          makes a guess at the home for Heritrix. It doesn't always guess
          correctly.</para>
        </sect4>

        <sect4>
          <title>JAVA_HOME</title>

          <para>This environment variable may already exist. It should point
          to the Java installation on the machine. An example of how this
          might be set (assuming your shell is bash):</para>

          <para><programlisting>  % export JAVA_HOME=/usr/local/java/jre/</programlisting></para>
        </sect4>

        <sect4 id="java_opts">
          <title>JAVA_OPTS</title>

          <para>Pass options to the Heritrix JVM by populating the JAVA_OPTS
          environment variable with values. For example, if you want to have
          Heritrix run with a larger heap, say 512 megs, you could do either
          of the following (assuming your shell is bash):<programlisting>  % export JAVA_OPTS="-Xmx512M"
% $HERITRIX_HOME/bin/heritrix</programlisting>Or, you could do it all on the
          one line as follows:<programlisting>  % JAVA_OPTS="-Xmx512m" $HERITRIX_HOME/bin/heritrix</programlisting></para>
        </sect4>
      </sect3>

      <sect3>
        <title>System properties</title>

        <para>Below we document the system properties passed on the
        command-line that can influence Heritrix's behavior. If you are using
        the /bin/heritrix script to launch Heritrix you may have to edit it to
        change/set these properties or else pass them as part of
        JAVA_OPTS.</para>

        <sect4 id="heritrix.properties">
          <title>heritrix.properties</title>

          <para>Set this property to point at an alternate heritrix.properties
          file -- <literal>e.g.:
          -Dheritrix.properties=/tmp/alternate.properties</literal> -- when
          you want heritrix to use a properties file other than that found at
          <literal>conf/heritrix.properties</literal>.</para>
        </sect4>

        <sect4>
          <title>heritrix.context</title>

          <para>Provide an alternate context for the Heritrix admin UI.
          Usually the admin webapp is mounted on root: i.e. '/'.</para>
        </sect4>

        <sect4>
          <title>heritrix.development</title>

          <para>Set this property when you want to run the crawler from
          eclipse. This property takes no arguments. When this property is
          set, the <literal>conf</literal> and <literal>webapps</literal>
          directories will be found in their development locations and startup
          messages will show on the text console (standard out).</para>
        </sect4>

        <sect4>
          <title>heritrix.home</title>

          <para>Where heritrix is homed usually passed by the heritrix launch
          script.</para>
        </sect4>

        <sect4>
          <title>heritrix.out</title>

          <para>Where stdout/stderr are sent, usually heritrix_out.log and
          passed by the heritrix launch script.</para>
        </sect4>

        <sect4>
          <title>heritrix.version</title>

          <para>Version of heritrix set by the heritrix build into
          heritrix.properties.</para>
        </sect4>

        <sect4>
          <title>heritrix.jobsdir</title>

          <para>Where to drop heritrix jobs. Usually empty. Default location
          is <literal>${HERITRIX_HOME}/jobs</literal>.</para>
        </sect4>

        <sect4 id="hconf">
          <title>heritrix.conf</title>

          <para>Specify an alternate configuration directory other than the
          default <literal>$HERITRIX_HOME/conf</literal>.</para>
        </sect4>

        <sect4>
          <title>heritrix.cmdline</title>

          <para>This set of system properties are rarely used. They are for
          use when Heritrix has NOT been started from the command-line -- e.g.
          its been embedded in another application -- and the startup
          configuration that is set usually by command-line options, instead
          needs to be done via system properties alone.</para>

          <sect5>
            <title>heritrix.cmdline.admin</title>

            <para>Value is a colon-delimited String user name and password for
            admin GUI</para>
          </sect5>

          <sect5>
            <title>heritrix.cmdline.nowui</title>

            <para>If set to true, will prevent embedded web server crawler
            control interface from starting up.</para>
          </sect5>

          <sect5>
            <title>heritrix.cmdline.order</title>

            <para>If set to to a string file path, will use the specified
            crawl order XML file.</para>
          </sect5>

          <sect5>
            <title>heritrix.cmdline.port</title>

            <para>Value is the port to run the GUI on.</para>
          </sect5>

          <sect5>
            <title>heritrix.cmdline.run</title>

            <para>If true, crawler is set into run mode on startup.</para>
          </sect5>
        </sect4>

        <sect4>
          <title>javax.net.ssl.trustStore</title>

          <para>Heritrix has its own trust store at
          <literal>conf/heritrix.cacerts</literal> that it uses if the
          <literal>FetcherHTTP</literal> is configured to use a trust level of
          other than <emphasis>open</emphasis> (open is the default setting).
          In the unusual case where you'd like to have Heritrix use an
          alternate truststore, point at the alternate by supplying the JSSE
          <literal>javax.net.ssl.trustStore</literal> property on the command
          line: e.g.</para>
        </sect4>

        <sect4>
          <title>java.util.logging.config.file</title>

          <para>The Heritrix <filename><literal>conf</literal></filename>
          directory includes a file named
          <filename>heritrix.properties</filename>. A section of this file
          specifies the default Heritrix logging configuration. To override
          these settings, point
          <literal>java.util.logging.config.file</literal> at a properties
          file with an alternate logging configuration. Below we reproduce the
          default <filename>heritrix.properties</filename> for
          reference:<programlisting>  # Basic logging setup; to console, all levels
handlers= java.util.logging.ConsoleHandler
java.util.logging.ConsoleHandler.level= ALL

# Default global logging level: only warnings or higher
.level= WARNING

# currently necessary (?) for standard logs to work
crawl.level= INFO
runtime-errors.level= INFO
uri-errors.level= INFO
progress-statistics.level= INFO
recover.level= INFO

# HttpClient is too chatty... only want to hear about severe problems
org.apache.commons.httpclient.level= SEVERE</programlisting>Here's an example
          of how you might specify an override:<programlisting>  % JAVA_OPTS="-Djava.util.logging.config.file=heritrix.properties" \
      ./bin/heritrix --no-wui order.xml</programlisting></para>

          <para>Alternatively you could edit the default file.</para>
        </sect4>

        <sect4>
          <title>java.io.tmpdir</title>

          <para>Specify an alternate tmp directory. Default is /tmp.</para>
        </sect4>

        <sect4>
          <title>com.sun.management.jmxremote.port</title>

          <para>What port to start up JMX Agent on. Default is 8849. See also
          the environment variable JMX_PORT.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="security">
      <title>Security Considerations</title>

      <para>The crawler is a large and active network application which
      presents security implications, both local to the machine where it
      operates, and remotely for machines it contacts.</para>

      <sect3>
        <title>Local to the Crawling Machine</title>

        <para>It is important to recognize that the web UI (discussed in <xref
        linkend="wui" />) and JMX agent (discussed in <xref
        linkend="mon_com" />) allow remote control of the crawler process in
        ways that might potentially disrupt a crawl, change the crawler's
        behavior, read or write locally-accessible files, and perform or
        trigger other actions in the Java VM or local machine.</para>

        <para>The administrative login and password are currently only a very
        mild protection against unauthorized access, unless you take
        additional steps to prevent access to the crawler machine. We strongly
        recommend some combination of the following practices:</para>

        <para><emphasis role="bold">First,</emphasis> use network
        configuration tools, like a firewall, to only allow trusted remote
        hosts to contact the web UI and, if applicable, JMX agent ports. (The
        default web UI port is 8080; JMX is 8849.)</para>

        <para><emphasis role="bold">Second,</emphasis> use a strong and unique
        username/password combination to secure the web UI and JMX agent.
        However, keep in mind that the default administrative web server uses
        plain HTTP for access, so these values are susceptible to
        eavesdropping in transit if network links between your browser and the
        crawler are compromised. (An upcoming update will change the default
        to HTTPS.) Also, setting the username/password on the command-line may
        result in their values being visible to other users of the crawling
        machine, and they are additionally printed to the console and
        heritrix_out.log for operator reference.</para>

        <para><emphasis role="bold">Third,</emphasis> run the crawler as a
        user with the minimum privileges necessary for its operation, so that
        in the event of unauthorized access to the web UI or JMX agent, the
        potential damage is limited.</para>

        <para>Successful unauthorized access to the web UI or JMX agent could
        trivially end or corrupt a crawl, or change the crawler's behavior to
        be a nuisance to other network hosts. By adjusting configuration
        paths, unauthorized access could potentially delete, corrupt, or
        replace files accessible to the crawler process, and thus cause more
        extensive problems on the crawler machine.</para>

        <para>Another potential risk is that some worst-case or
        maliciously-crafted crawled content might, in combination with crawler
        bugs, disrupt the crawl or other files or operations of the local
        system. For example, in the past, even without malicious intent, some
        rich-media content has caused runaway memory use in 3rd-party
        libraries used by the crawler, resulting in a memory-exhaustion
        condition that can stop or corrupt a crawl in progress. Similarly,
        atypical input patterns have at times caused runaway CPU use by
        crawler link-extraction regular expressions, severely slowing crawls.
        Crawl operators should monitor their crawls closely and stay informed
        via the project discussion list and bug database for any newly
        discovered similar bugs.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="wui">
    <title>Web based user interface</title>

    <para>After Heritrix has been launched from the command line, the web
    based user interface (WUI) becomes accessible.</para>

    <para>The URI to access the WUI is printed on the text console from which
    the program was launched (typically
    <literal>http://&lt;host&gt;:8080/admin/</literal>).</para>

    <para>The WUI is password protected. There is no default login for access;
    one must be specified using either the '-a'/'--admin' command-line option
    at startup or by setting the 'heritrix.cmdline.admin' system property. The
    currently valid username and password combination will be printed out to
    the console, along with the access URL for the WUI, at startup.</para>

    <para>The WUI can be accessed via any web browser. While we've endeavoured
    to make certain that it functions in all recent browsers, Mozilla 5 or
    newer is recommended. IE 6 or newer should also work without
    problems.</para>

    <para>The initial login page takes the username/password combination
    discussed above. Logins will time out after a period of non-use.</para>

    <caution>
      <para>By default, communication with the WUI is not done over an
      encrypted HTTPS connection! Passwords will be submitted over the network
      in plain text, so you should take additional steps to protect your
      crawler administrative interface from unauthorized access, as described
      in the <xref linkend="security" /> section.</para>
    </caution>
  </sect1>

  <sect1 id="tutorial">
    <title>A quick guide to running your first crawl job</title>

    <para>Once you've installed Heritrix and logged into the WUI (see above)
    you are presented with the web Console page. Near the top there is a row
    of tabs.</para>

    <para><emphasis role="bold">Step 1.</emphasis> <emphasis>Create a
    job</emphasis></para>

    <para>To create a new job choose the Jobs tab, this will take you to the
    Jobs page. Once there you are presented with three options for creating a
    new job. Select 'With defaults'. This will create a new job based on the
    default profile (see <xref linkend="profile" />).</para>

    <para>On the screen that comes next you will be asked to supply a name,
    description and a seed list for the new job.</para>

    <para>For a name supply a short text with no special characters or spaces
    (except dash and underscore). You can skip the description if you like. In
    the seeds list type in the URL of the sites you are interested in
    harvesting. One URL to a line.</para>

    <para>Creating a job is covered in greater detail in <xref
    linkend="creating" />.</para>

    <para><emphasis role="bold">Step 2.</emphasis> <emphasis>Configure the
    job</emphasis></para>

    <para>Once you've entered this information in you are ready to go to the
    configuration pages. Click the <emphasis>Modules</emphasis> button in the
    row of buttons at the bottom of the page.</para>

    <para>This will take you to the modules configuration page (more details
    in <xref linkend="modules" />). For now we are only interested in the
    option second from the top named <emphasis role="bold">Select crawl
    scope</emphasis>. It allows you to specify the limits of the crawl. By
    default it is limited to the domains that your seeds span. This may be
    suitable for your purposes. If not you can choose a broad scope (not
    limited to the domains of its seeds) or the more restrictive host scope
    that limits the crawl to the hosts that its seeds span. For more on scopes
    refer to <xref linkend="scopes" />.</para>

    <para>To change scopes, select the new one from the combobox and click the
    <emphasis>Change </emphasis>button.</para>

    <para>Next turn your attention to the second row of tabs at the top of the
    page, below the usual tabs. You are currently on the far left tab. Now
    select the tab called <emphasis>Settings</emphasis> near the middle of the
    row.</para>

    <para>This takes you to the Settings page. It allows you to configure
    various details of the crawl. Exhaustive coverage of this page can be
    found in <xref linkend="settings" />. For now we are only interested in
    the two settings under <emphasis role="bold">http-headers</emphasis>.
    These are the <literal>user-agent</literal> and <literal>from</literal>
    field of the HTTP headers in the crawlers requests. You must set them to
    valid values before a crawl can be run. The current values upper-case what
    needs replacing. If you have trouble with that please refer to <xref
    linkend="httpheaders" /> for what's regarded as valid values.</para>

    <para>Once you've set the <emphasis role="bold">http-headers</emphasis>
    settings to proper values (and made any other desired changes), you can
    click the <emphasis>Submit job</emphasis> tab at the far right of the
    second row of tabs. The crawl job is now configured and ready to
    run.</para>

    <para>Configuring a job is covered in greater detail in <xref
    linkend="config" />.</para>

    <para><emphasis role="bold">Step 3.</emphasis> <emphasis>Running the
    job</emphasis></para>

    <para>Submitted new jobs are placed in a queue of pending jobs. The
    crawler does not start processing jobs from this queue until the crawler
    is started. While the crawler is stopped, jobs are simply held.</para>

    <para>To start the crawler, click on the Console tab. Once on the Console
    page, you will find the option <emphasis>Start</emphasis> at the top of
    the <emphasis role="bold">Crawler Status</emphasis> box, just to the right
    of the indicator of current status. Clicking this option will put the
    crawling into <emphasis>Crawling Jobs</emphasis> mode, where it will begin
    crawling any next pending job, such as the job you just created and
    configured.</para>

    <para>The Console will update to display progress information about the
    on-going crawl. Click the <emphasis>Refresh</emphasis> option (or the
    top-left Heritrix logo) to update this information.</para>

    <para>For more information about running a job see <xref
    linkend="running" />.</para>

    <para>Detailed information about evaluating the progress of a job can be
    found in <xref linkend="analysis" />.</para>
  </sect1>

  <sect1 id="creating">
    <title>Creating jobs and profiles</title>

    <para>In order to run a crawl a configuration must be created that defines
    it. In Heritrix such a configuration is called a <emphasis
    role="bold">crawl job</emphasis>.</para>

    <sect2>
      <title id="crawljob">Crawl job</title>

      <para>A crawl job encompasses the configurations needed to run a single
      crawl. It also contains some additional elements such as file locations,
      status etc.</para>

      <para>Once logged onto the WUI new jobs can be created by going to the
      <emphasis>Jobs</emphasis> tab. Once the Jobs page loads users can create
      jobs by choosing of the following three options:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Based on existing job</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          any existing job, regardless of whether it has been crawled or not.
          Can be useful for repeating crawls or recovering a crawl that had
          problems. (See <xref linkend="recover" /></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Based on a profile</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          any existing profiles.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">With defaults</emphasis></para>

          <para>This option creates a new crawl job based on the default
          profile.</para>
        </listitem>
      </orderedlist>

      <para>Options 1 and 2 will display a list of available options.
      Initially there are two profiles and no existing jobs.</para>

      <para>All crawl jobs are created by basing them on profiles (see <xref
      linkend="profile" />) or existing jobs.</para>

      <para>Once the proper profile/job has been chosen to base the new job
      on, a simple page will appear asking for the new job's:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Name</emphasis></para>

          <para>The name must only contain letters, numbers, dash (-) and
          underscore (_). No other characters are allowed. This name will be
          used to identify the crawl in the WUI but it need not be unique. The
          name can not be changed later</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Description</emphasis></para>

          <para>A short description of the job. This is a freetext input box
          and can be edited later.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Seeds</emphasis></para>

          <para>The seed URIs to use for the job. This list can be edited
          later along with the general configurations.</para>
        </listitem>
      </orderedlist>

      <para>Below these input fields there are several buttons. The last one
      <emphasis>Submit job</emphasis> will immediately submit the job and
      (assuming it is properly configured) it will be ready to run (see <xref
      linkend="running" />). The other buttons will take the user to the
      relevant configuration pages (those are covered in detail in <xref
      linkend="config" />). Once all desired changes have been made to the
      configuration, click the '<emphasis>Submit job</emphasis>' tab (usually
      displayed top and bottom right) to submit it to the list of waiting
      jobs.<note>
          <para>Changes made afterwards to the original jobs or profiles that
          a new job is based on will <emphasis role="bold">not</emphasis> in
          any way affect the newly created job.</para>
        </note><note>
          <para>Jobs based on the default profile provided with Heritrix are
          not ready to run <emphasis>as is</emphasis>. Their HTTP header
          information must be set to valid values. See <xref
          linkend="httpheaders" /> for details.</para>
        </note></para>
    </sect2>

    <sect2>
      <title id="profile">Profile</title>

      <para>A profile is a template for a crawl job. It contains all the
      configurations that a crawl job would, but is not considered to be
      'crawlable'. That is Heritrix will not allow you to directly crawl a
      profile, only jobs based on profiles. The reason for this is that while
      profiles may in fact be complete, they may also not be.</para>

      <para>A common example is leaving the HTTP headers
      (<literal>user-agent</literal>, <literal>from</literal>) in an illegal
      state in a profile to force the user to input valid data. This applies
      to the default (<emphasis>default</emphasis>) profile that comes with
      Heritrix. Other examples would be leaving the seeds list empty, not
      specifying some processors (such as the writer/indexer) etc.</para>

      <para>In general there is less error checking of profiles.</para>

      <para>To manage profiles, go to the <emphasis>Profiles</emphasis> tab in
      the WUI. That page will display a list of existing profiles. To create a
      new profile select the option of creating a "New profile based on it"
      from the existing profile to use as a template. Much like jobs, profiles
      can only be created based on other profiles. It is not possible to
      create profiles based on existing jobs.</para>

      <para>The process from there on mirrors the creation of jobs. A page
      will ask for the new profiles name, description and seeds list. Unlike
      job names, profile names <emphasis>must be unique</emphasis> from other
      profile names - jobs and a profile can share the same name - otherwise
      the same rules apply.</para>

      <para>The user then proceeds to the configuration pages (see <xref
      linkend="config" />) to modify the behavior of the new profile from that
      of the parent profile.<note>
          <para>Even though profiles are based on other profiles, changes made
          to the original profiles afterwards will <emphasis
          role="bold">not</emphasis> affect the new ones.</para>
        </note></para>
    </sect2>
  </sect1>

  <sect1 id="config">
    <title>Configuring jobs and profiles</title>

    <para>Creating crawl jobs (<xref linkend="crawljob" />) and profiles
    (<xref linkend="profile" />) is just the first step. Configuring them is a
    more complicated process.</para>

    <para>The following section applies equally to configuring crawl jobs and
    profiles. It does not matter if new ones are being created or existing
    ones are being edited. The interface is almost entirely the same, only the
    <emphasis>Submit job</emphasis> / <emphasis>Finished</emphasis> button
    will vary.<note>
        <para>Editing options for jobs being crawled are somewhat limited. See
        <xref linkend="editrun" /> for more.</para>
      </note></para>

    <para>Each page in the configuration section of the WUI will have a
    secondary row of tabs below the general ones. This secondary row is often
    replicated at the bottom of longer pages.</para>

    <para>This row offers access to different parts of the configuration.
    While configuring the global level (more on global vs. overrides and
    refinements in <xref linkend="overrides" /> and <xref
    linkend="refinements" />) the following options are available (left to
    right):</para>

    <itemizedlist>
      <listitem>
        <para>Modules (<xref linkend="modules" />)</para>

        <para>Add/remove/set configurable modules, such as the crawl Scope
        (<xref linkend="scopes" />), Frontier (<xref linkend="frontier" />),
        or Processors (<xref linkend="processors" />).</para>
      </listitem>

      <listitem>
        <para>Submodules (<xref linkend="submodules" />)</para>

        <para>Here you can:</para>

        <itemizedlist>
          <listitem>
            <para>Add/remove/reorder URL canonicalization rules (<xref
            linkend="urlcanon" />)</para>
          </listitem>

          <listitem>
            <para>Add/remove/reorder filters (<xref
            linkend="filters" />)</para>
          </listitem>

          <listitem>
            <para>Add/remove login credentials (<xref
            linkend="credentials" />)</para>
          </listitem>
        </itemizedlist>
      </listitem>

      <listitem>
        <para>Settings (<xref linkend="settings" />)</para>

        <para>Configure settings on Heritrix modules</para>
      </listitem>

      <listitem>
        <para>Overrides (<xref linkend="overrides" />)</para>

        <para>Override settings on Heritrix modules based on domain</para>
      </listitem>

      <listitem>
        <para>Refinements (<xref linkend="refinements" />)</para>

        <para>Refine settings on Heritrix modules based on arbitrary
        criteria</para>
      </listitem>

      <listitem>
        <para>Submit job / Finished</para>

        <para>Clicking this tab will take the user back to the Jobs or
        Profiles page, saving any changes.</para>
      </listitem>
    </itemizedlist>

    <para>The <emphasis>Settings</emphasis> tab is probably the most
    frequently used page as it allows the user to fine tune the settings of
    any Heritrix module used in a job or profile.</para>

    <para>It is safe to navigate between these, it will not cause new jobs to
    be submitted to the queue of pending jobs. That only happens once the
    <emphasis>Submit job</emphasis> tab is clicked. Navigating out of the
    configuration pages using the top level tabs will cause new jobs to be
    lost. Any changes made are saved when navigating within the configuration
    pages. There is no undo function, once made changes can not be
    undone.</para>

    <sect2 id="modules">
      <title>Modules (Scope, Frontier, and Processors)</title>

      <para>Heritrix has several types of pluggable modules. These modules,
      while having a fixed interface usually have a number of provided
      implementations. They can also be third party plugins. The "Modules" tab
      allows the user to set several types of these pluggable modules.</para>

      <para>Once modules have been added to the configuration they can be
      configured in greater detail on the Settings tab (<xref
      linkend="settings" />). If a module can contain within it multiple other
      modules, these can be configured on the Submodules tab.</para>

      <para><note>
          <para>Modules are referred to by their Java class names
          (org.archive.crawler.frontier.BdbFrontier). This is done because
          these are the only names we can be assured of being unique.</para>
        </note>See <ulink
      url="http://crawler.archive.org/articles/developer_manual/index.html">Developer's
      Manual</ulink> for information about creating and adding custom modules
      to Heritrix.</para>

      <sect3 id="scopes">
        <title>Crawl Scope</title>

        <para>A crawl scope is an object that decides for each discovered URI
        if it is within the scope of the current crawl.</para>

        <para>Several scopes are provided with Heritrix:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">BroadScope</emphasis></para>

            <para>This scope allows for limiting the depth of a crawl (how
            many links away Heritrix should crawl) but does not impose any
            limits on the hosts, domains, or URI paths crawled.</para>
          </listitem>

          <listitem id="surtprefixscope">
            <para><emphasis role="bold">SurtPrefixScope</emphasis></para>

            <para>A highly flexible and fairly efficient scope which can crawl
            within defined domains, individual hosts, or path-defined areas of
            hosts, or any mixture of those, depending on the
            configuration.</para>

            <para>It considers whether any URI is inside the primary focus of
            the scope by converting the URI to its <xref linkend="surt" />
            form, and then seeing if that SURT form begins with any of a
            number of <xref linkend="surtprefix" />es. (See the glossary
            definitions for detailed information about the SURT form of a URI
            and SURT prefix comparisons.)</para>

            <para>The operator may establish the set of SURT prefixes used
            either by letting the SURT prefixes be implied from the supplied
            seed URIs, specifying an external file with a listing of SURT
            prefixes, or both.</para>

            <para>This scope also enables a special syntax within the seeds
            list for adding SURT prefixes separate from seeds. Any line in the
            seeds list beginning with a '+' will be considered a SURT prefix
            specification, rather than a seed. Any URL you put after the '+'
            will only be used to deduce a SURT prefix -- it will not be
            independently scheduled. You can also put your own literal SURT
            prefix after the '+'.</para>

            <para>For example, each of the following SURT prefix directives in
            the seeds box are equivalent:</para>

            <para><programlisting>
+http://(org,example,      # literal SURT prefix
+http://example.org        # regular URL implying same SURT prefix
+example.org               # URL fragment with implied 'http' scheme
            </programlisting></para>

            <para>When you use this scope, it adds 3 hard-to-find-in-the-UI
            attributes -- <literal>surts-source-file</literal>,
            <literal>seeds-as-surt-prefixes</literal>, and
            <literal>surts-dump-file</literal> -- to the end of the scope
            section, just after <literal>transitiveFilter</literal> but before
            <literal>http-headers</literal>.</para>

            <para>Use the <literal>surts-source-file</literal> setting to
            supply an external file from which to infer SURT prefixes, if
            desired. Any URLs in this file will be converted to the implied
            SURT prefix, and any line beginning with a '+' will be interpreted
            as a literal, precise SURT prefix. Use the
            <literal>seeds-as-surt-prefixes</literal> setting to establish
            whether SURT prefixes should be deduced from the seeds, in
            accordance with the rules given at the <xref
            linkend="surtprefix" /> glossary entry. (The default is 'true', to
            deduce SURT prefixes from seeds.)</para>

            <para>To see what SURT prefixes were actually used -- perhaps
            merged from seed-deduced and externally-supplied -- you can
            specify a file path in the <literal>surts-dump-file</literal>
            setting. The sorted list of actual SURT prefixes used will be
            written to that file for reference. (Note that redundant entries
            will be removed from this dump. If you have SURT prefixes
            &lt;http://(org,&gt; and &lt;http://(org,archive,&gt;, only the
            former will actually be used, because all SURT form URIs prefixed
            by the latter are also prefixed by the former.)</para>

            <para>See also the crawler wiki on <ulink
            url="http://crawler.archive.org/cgi-bin/wiki.pl?SurtScope">SurtScope</ulink>.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">FilterScope</emphasis></para>

            <para>A highly configurable scope. By adding different filters in
            different combinations this scope can be configured to provide a
            wide variety of behaviour.</para>

            <para>After selecting this filter, you must then go to the
            <emphasis>Filters</emphasis> tab and add the filters you want to
            run as part of your scope. Add the filters at the
            <emphasis>focusFilter</emphasis> label and give them a meaningful
            name. The URIRegexFilter probably makes most sense in this context
            (The ContentTypeRegexFilter won't work at scope time because we
            don't know the content-type till after we've fetched the
            document).</para>

            <para>After adding the filter(s), return to the
            <emphasis>Settings</emphasis> tab and fill in any configuration
            required of the filters. For example, say you added the
            URIRegexFilter, and you wanted only 'www.archive.org' hosts to be
            in focus, fill in a regex like the following:
            <literal>^(?:http|dns)www.archve.org/\.*</literal> (Be careful you
            don't rule out prerequisites such as dns or robots.txt when
            specifying your scope filter).</para>
          </listitem>
        </itemizedlist>

        <para>The following scopes are available, but the same effects can be
        achieved more efficiently, and in combination, with SurtPrefixScope.
        When SurtPrefixScope can be more easily understood and configured,
        these scopes may be removed entirely.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">DomainScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of domains
            defined by the provided seeds. That is any URI discovered
            belonging to a domain from which one of the seed came is within
            scope. Like always it is possible to apply depth
            restrictions.</para>

            <para>Using the seed 'archive.org', a domain scope will fetch
            'audio.archive.org', 'movies.archive.org', etc. It will fetch all
            discovered URIs from 'archive.org' and from any subdomain of
            'archive.org'.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">HostScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of hosts
            defined by the provided seeds.</para>

            <para>If the seed is 'www.archive.org', then we'll only fetch
            items discovered on this host. The crawler will not go to
            'audio.archive.org' or 'movies.archive.org'.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PathScope</emphasis></para>

            <para>This scope goes yet further and limits the discovered URIs
            to a section of paths on hosts defined by the seeds. Of course any
            host that has a seed pointing at its root (i.e.
            <literal>www.sample.com/index.html</literal>) will be included in
            full where as a host whose only seed is
            <literal>www.sample2.com/path/index.html</literal> will be limited
            to URIs under <literal>/path/</literal>.</para>

            <note>
              <para>Internally Heritrix defines everything up to the right
              most slash as the <literal>path</literal> when doing path scope
              so for example, the URLs
              <literal>http://members.aol.com/bigbird</literal> and
              <literal>http://members.aol.com/~bigbird</literal> will treat as
              in scope any URL that begins <literal>members.aol.com</literal>.
              If your intent is to only include all below the path
              <literal>bigbird</literal>, add a slash on the end, using a form
              such as <literal>http://members.aol.com/bigbird/</literal> or
              <literal>http://members.aol.com/bigbird/index.html</literal>
              instead.</para>
            </note>
          </listitem>
        </itemizedlist>

        <para>Scopes usually allow for some flexibility in defining depth and
        possible transitive includes (that is getting items that would usually
        be out of scope because of special circumstance such as their being
        embedded in the display of an included resource). Most notably, every
        scope can have additional filters applied in two different contexts
        (some scopes may only have one these contexts).</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Focus</emphasis></para>

            <para>URIs matching these filters will be considered to be within
            scope</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Exclude</emphasis></para>

            <para>URIs matching these filters will be considered to be out of
            scope.</para>
          </listitem>
        </orderedlist>

        <para>Custom made Scopes may have different sets of filters. Also some
        scopes have filters hardcoded into them. This allows you to edit their
        settings but not remove or replace them. For example most of the
        provided scopes have a <literal>Transclusion</literal> filter
        hardcoded into them that handles transitive items (URIs that normally
        shouldn't be included but because of special circumstance they will be
        included).</para>

        <para>For more about Filters see <xref linkend="filters" />.</para>

        <sect4 id="scopeproblems">
          <title>Problems with the current Scopes</title>

          <para>Our original Scope classes -- PathScope, HostScope,
          DomainScope, BroadScope -- all could be thought of as fitting a
          specific pattern: A URI is included if and only if:</para>

          <para><programlisting>protected final boolean innerAccepts(Object o) {
    return ((isSeed(o) || focusAccepts(o)) || additionalFocusAccepts(o) ||
            transitiveAccepts(o)) &amp;&amp; !excludeAccepts(o);
}</programlisting></para>

          <para>More generally, the <emphasis>focus</emphasis> filter was
          meant to rule things in by prima facia/regexp-pattern analysis; the
          <emphasis>transitive</emphasis> filter rule extra items in by
          dynamic path analysis (for example, off site embedded images); and
          the <emphasis>exclusion</emphasis> filter rule things out by any
          number of chained exclusion rules. So in a typical crawl, the
          <emphasis>focus</emphasis> filter drew from one of these
          categories:<itemizedlist>
              <listitem>
                <para><emphasis role="bold">broad</emphasis> : accept
                all</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">domain</emphasis>: accept if on
                same 'domain' (for some definition) as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">host</emphasis>: accept if on
                exact host as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">path</emphasis>: accept if on same
                host and a shared path-prefix as seeds</para>
              </listitem>
            </itemizedlist>The <emphasis>transitive</emphasis> filter configuration
          was based on the various link-hops and embed-hop thresholds
          set by the operator.</para>

          <para>The <emphasis>exclusion</emphasis> filter was in fact a
          compound chain of filters, OR'ed together, such that any one of them
          could knock a URI out of consideration. However, a number of aspects
          of this arrangement have caused problems: <orderedlist>
              <listitem>
                <para>To truly understand what happens to an URI, you must
                understand the above nested boolean-construct.</para>
              </listitem>

              <listitem>
                <para>Adding mixed focuses -- such as all of this one host,
                all of this other domain, and then just these paths on this
                other host -- is not supported by these classes, nor easy to
                mix-in to the <emphasis>focus</emphasis> filter.</para>
              </listitem>

              <listitem>
                <para>Constructing and configuring the multiple filters
                required many setup steps across several WUI pages.</para>
              </listitem>

              <listitem>
                <para>The reverse sense of the <emphasis>exclusion</emphasis>
                filters -- if URIs are accepted by the filter, they are
                excluded from the crawl -- proved confusing, exacerbated by
                the fact that 'filter' itself can commonly mean either 'filter
                in' or 'filter out'.</para>
              </listitem>
            </orderedlist></para>

          <para>As a result of these problems, the SurtPrefixScope was added,
          and further major changes are planned. The first steps are described
          in the next section, <xref linkend="decidingscope" />. These changes
          will also affect whether and how filters (see <xref
          linkend="filters" />) are used.</para>
        </sect4>

        <sect4 id="decidingscope">
          <title>DecidingScope</title>

          <para>To address the shortcomings above, and generally make
          alternate scope choices more understandable and flexible, a new
          mechanism for scoping and filtering has been introduced in Heritrix
          1.4. This new approach is somewhat like (and inspired by) HTTrack's
          'scan rules'/filters, Alexa's mask/ignore/void syntax for adjusting
          recurring crawls, or the Nutch 'regex-urlfilter' facility, but may
          be a bit more general than any of those.</para>

          <para>This new approach is available as a DecidingScope, which is
          modelled as a series of DecideRules. Each DecideRule, when presented
          with an Object (most often a URI of some form), may respond with one
          of three decisions:</para>

          <itemizedlist>
            <listitem>
              <para>ACCEPT: the object is ruled in</para>
            </listitem>

            <listitem>
              <para>REJECT: the object is ruled out</para>
            </listitem>

            <listitem>
              <para>PASS: the rule has no opinion; retain whatever previous
              decision was made</para>
            </listitem>
          </itemizedlist>

          <para>To define a Scope, the operator configures an ordered series
          of DecideRules. A URI under consideration begins with no assumed
          status. Each rule is applied in turn to the candidate URI. If the
          rule decides ACCEPT or REJECT, the URI's status is set accordingly.
          After all rules have been applied, if the URI's status is ACCEPT it
          is "in scope" and scheduled for crawling; if its status is REJECT it
          is discarded.</para>

          <para>There are no branches, but much of what nested conditionals
          can achieve is possible, in a form that should be be easier to
          follow than arbitrary expressions.</para>

          <para>The current list of available DecideRules includes:</para>

          <para><programlisting>   
    AcceptDecideRule -- ACCEPTs all (establishing an early default)
    RejectDecideRule -- REJECTs all (establishing an early default)
    TooManyHopsDecideRule(max-hops=N) -- REJECTS all with hopsPath.length()&gt;N, PASSes otherwise
    PrerequisiteAcceptDecideRule -- ACCEPTs any with 'P' as last hop, PASSes otherwise (allowing prerequisites of items within other limits to also be included
    MatchesRegExpDecideRule(regexp=pattern) -- ACCEPTs (or REJECTs) all matching a regexp, PASSing otherwise
    NotMatchesRegExpDecideRule(regexp=pattern) -- ACCEPTs (or REJECTs) all *not* matching a regexp, PASSing otherwise. 
    PathologicalPathDecideRule(max-reps=N) -- REJECTs all mathing problem patterns
    TooManyPathSegmentsDecideRule(max-segs=N) -- REJECTs all with too many path-segments ('/'s)
    TransclusionDecideRule(extra-hops=N) -- ACCEPTs anything with up to N non-navlink (non-'L')hops at end
    SurtPrefixedDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything matched by SURT prefix set generated from supplied seeds/files/etc.
    NotSurtPrefixedDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything *not* matched by SURT prefix set generated from supplied seeds/files/etc.
    OnHostsDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything on same hosts as deduced from supplied seeds/files/etc.
    NotOnHostsDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything on *not* same hosts as deduced from supplied seeds/files/etc.
    OnDomainsDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything on same domains as deduced from supplied seeds/files/etc.
    NotOnDomainsSetDecideRule(use-seeds=bool;use-file=path) -- ACCEPTs (or REJECTs) anything *not* on same domains as deduced from supplied seeds/files/etc.
    MatchesFilePatternDecideRule -- ACCEPTs (or REJECTs) URIs matching a chosen predefined convenience regexp pattern (such as common file-extensions)
    NotMatchesFilePatternDecideRule -- ACCEPTs (or REJECTs) URIs *not* matching a chosen predefined convenience regexp pattern
        </programlisting></para>

          <para>...covering just about everything our previous focus- and
          filter- based classes did. By ordering exclude and include actions,
          combinations that were awkward before -- or even impossible without
          writing custom code -- becomes straightforward.</para>

          <para>For example, a previous request that was hard for us to
          accomodate was the idea: "crawl exactly these X hosts, and get
          offsite images if only on the same domains." That is, don't wander
          off the exact hosts to follow navigational links -- only to get
          offsite resources that share the same domain.</para>

          <para>Our relevant function-of-seeds tests -- host-based and
          domain-based -- were exclusive of each other (at the 'focus' level)
          and difficult to mix-in with path-based criteria (at the
          'transitive' level).</para>

          <para>As a series of DecideRules, the above request can be easily
          achieved as:</para>

          <para><programlisting>
        RejectDecideRule
        OnHostsDecideRule(use-seeds=true)
        TranscludedDecideRule(extra-hops=2)
        NotOnDomainsDecideRule(REJECT,use-seeds=true);
        </programlisting></para>

          <para>A good default set of DecideRules for many purposes would
          be...</para>

          <para><programlisting>
        RejectDecideRule               // reject by default
        SurtPrefixedDecideRule         // accept within SURT prefixes established by seeds
        TooManyHopsDecideRule          // but reject if too many hops from seeds
        TransclusionDecideRule         // notwithstanding above, accept if within a few transcluding hops (frames/imgs/redirects)
        PathologicalPathDecideRule     // but reject if pathological repetitions
        TooManyPathSegmentsDecideRule  // ...or if too many path-segments
        PrerequisiteAcceptDecideRule   // but always accept a prerequisite of other URI
        </programlisting></para>

          <para>In Heritirx 1.10.0, the default profile was changed to use the
          above set of DecideRules (Previous to this, the operator had to
          choose the 'deciding-default' profile, since removed).</para>

          <para>The naming, behavior, and user-interface for DecideRule-based
          scoping is subject to significant change based on feedback and
          experience in future releases.</para>

          <para>Enable FINE logging on the class
          <literal>org.archive.crawler.deciderules.DecideRuleSequence</literal>
          to watch each deciderules finding on each processed URI.</para>
        </sect4>
      </sect3>

      <sect3 id="frontier">
        <title>Frontier</title>

        <para>The Frontier is a pluggable module that maintains the internal
        state of the crawl. What URIs have been discovered, crawled etc. As
        such its selection greatly effects, for instance, the order in which
        discovered URIs are crawled.</para>

        <para>There is only one Frontier per crawl job.</para>

        <para>Multiple Frontiers are provided with Heritrix, each of a
        particular character.</para>

        <sect4 id="bdbfrontier">
          <title>BdbFrontier</title>

          <para>The default Frontier in Heritrix as of 1.4.0 and later is the
          BdbFrontier(Previously, the default was the <xref linkend="hqf" />).
          The BdbFrontier visits URIs and sites discovered in a generally
          breadth-first manner, it offers configuration options controlling
          how it throttles its activity against particular hosts, and whether
          it has a bias towards finishing hosts in progress ('site-first'
          crawling) or cycling among all hosts with pending URIs.</para>

          <para>Discovered URIs are only crawled once, except that robots.txt
          and DNS information can be configured so that it is refreshed at
          specified intervals for each host.</para>

          <para>The main difference between the BdbFrontier and its precursor,
          <xref linkend="hqf" />, is that BdbFrontier uses BerkeleyDB Java
          Edition to shift more running Frontier state to disk.</para>
        </sect4>

        <sect4 id="hqf">
          <title>HostQueuesFrontier</title>

          <para>The forerunner of the <xref linkend="bdbfrontier" />. Now
          deprecated mostly because its custom disk-based data structures
          could not move as much Frontier state out of main memory as the
          BerkeleyDB Java Edition approach. Has same general characteristics
          as the <xref linkend="bdbfrontier" />.</para>
        </sect4>

        <sect4 id="dsf">
          <title>DomainSensitveFrontier</title>

          <para>A subclass of the <xref linkend="hqf" /> written by Oskar
          Grenholm. The DSF allows specifying an upper-bound on the number of
          documents downloaded per-site. It does this by exploiting <xref
          linkend="overrides" /> adding a filter to block further fetching
          once the crawler has attained per-site limits.</para>
        </sect4>

        <sect4 id="arf">
          <title>AdaptiveRevisitingFrontier</title>

          <para>The AdaptiveRevisitingFrontier -- a.k.a
          <emphasis>AR</emphasis> Frontier -- will repeatedly visit all
          encountered URIs. Wait time between visits is configurable and
          varies based on wait intervals specified by a WaitEvaluator
          processor. It was written by Kristinn Sigurdsson. <note>
              <para>This Frontier is still experimental, in active development
              and has not been tested extensively.</para>
            </note></para>

          <para>In addition to the WaitEvaluator (or similar processor) a
          crawl using this Frontier will also need to use the ChangeEvaluator
          processor: i.e. this Frontier requires that ChangeEvaluator and
          WaitEvaluator or equivalents are present in the processing
          chain.</para>

          <para>ChangeEvaluator should be at the very top of the extractor
          chain.</para>

          <para>WaitEvaluator -- or an equivalent -- needs to be in the post
          processing chain.</para>

          <para>The ChangeEvaluator has no configurable settings. The
          WaitEvaluator however has numerous settings to adjust the revisit
          policy. <itemizedlist>
              <listitem>
                <para>Initial wait. A waiting period before revisiting the
                first time.</para>
              </listitem>

              <listitem>
                <para>Increase and decrease factors on unchanged and changed
                documents respectively. Basically if a document has not
                changed between visits, its wait time will be multiplied by
                the "unchanged-factor" and if it has changed, the wait time
                will be divided by the "changed-factor". Both values accept
                real numbers, not just integers.</para>
              </listitem>

              <listitem>
                <para>Finally, there is a 'default-wait-interval' for URIs
                where it is not possible to judge changes in content.
                Currently this applies only to DNS lookups.</para>
              </listitem>
            </itemizedlist></para>

          <para>If you want to specify different wait times and factors for
          URIs based on their mime types, this is possible. You have to create
          a Refinement (<xref linkend="refinements" />) and use the
          ContentType criteria. Simply use a regular expression that matches
          the desired mime type as its parameter and then override the
          applicable parameters in the refinement.</para>

          <para>By setting the 'state' directory to the same location that
          another AR crawl used, it should resume that crawl (minus some
          stats).</para>
        </sect4>
      </sect3>

      <sect3 id="processors">
        <title id="processingchains">Processing Chains</title>

        <para>When a URI is crawled it is in fact passed through a series of
        processors. This series is split for convenience between five chains
        and the user can add, remove and reorder the processors on each of
        these chains.</para>

        <para>Each URI taken off the Frontier queue runs through the
        <literal>Processing Chains</literal> listed in the diagram shown
        below. URIs are always processed in the order shown in the diagram
        unless a particular processor throws a fatal error or decides to stop
        the processing of the current URI for some reason. In this
        circumstance, processing skips to the end, to the Post-processing
        chain, for cleanup.</para>

        <para>Each processing chain is made up of zero or more individual
        processors. For example, the extractor processing chain might comprise
        the <literal>ExtractorHTML</literal> , an
        <literal>ExtractorJS</literal> , and the
        <literal>ExtractorUniversal</literal> processors. Within a processing
        step, the order in which processors are run is the order in which
        processors are listed on the modules page.</para>

        <para>Generally, particular processors only make sense within the
        context of one particular processing chain. For example, it wouldn't
        make sense to run the <literal>FetchHTTP</literal> processor in the
        Post-processing chain. This is however not enforced, so users must
        take care to construct logical processor chains.</para>

        <graphic fileref="../processing_steps.png" format="PNG" />

        <para>Most of the processors are fairly self explanatory, however the
        first and last two merit a little bit more attention.</para>

        <para>In the <literal>Pre-fetch processing</literal> chain the
        following two processors should be included (or replacement modules
        that perform similar operations):</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Preselector</emphasis></para>

            <para>Last check if the URI should indeed be crawled. Can for
            example recheck scope. Useful if scope has been changed after the
            crawl starts (This processor is not strictly necessary).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PreconditionEnforcer</emphasis></para>

            <para>Ensures that all preconditions for crawling a URI have been
            met. These currently include verifying that DNS and robots.txt
            information has been fetched for the URI. Should always be
            included.</para>
          </listitem>
        </itemizedlist>

        <para>Similarly the <literal>Post Processing</literal> chain has the
        following special purpose processors:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">CrawlStateUpdater</emphasis></para>

            <para>Updates the per-host information that may have been affected
            by the fetch. This is currently robots and IP address info. Should
            always be included.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">LinksScoper</emphasis></para>

            <para>Checks all links extracted from the current download against
            the crawl scope. Those that are out of scope are discarded.
            Logging of discarded URLs can be enabled.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">FrontierScheduler</emphasis></para>

            <para>'Schedules' any URLs stored as CandidateURIs found in the
            current CrawlURI with the frontier for crawling. Also schedules
            prerequisites if any.</para>
          </listitem>
        </itemizedlist>
      </sect3>

      <sect3 id="stattrack">
        <title>Statistics Tracking</title>

        <para>Any number of statistics tracking modules can be attached to a
        crawl. Currently only one is provided with Heritrix. The
        <literal>StatisticsTracker</literal> module that comes with Heritrix
        writes the <literal>progress-statistics.log</literal> file and
        provides the WUI with the data it needs to display progress
        information about a crawl. It is strongly recommended that any crawl
        running with the WUI use this module.</para>
      </sect3>
    </sect2>

    <sect2 id="submodules">
      <title>Submodules</title>

      <para>On the Submodules tab, configuration points that take
      variable-sized listings of components can be configured. Components can
      be added, ordered, and removed. Examples of such components are listings
      of canonicalization rules to run against each URL discovered, <xref
      linkend="filters" /> on processors, and credentials. Once submodules are
      added under the Submodules tab, they will show in subsequent redrawings
      of the Settings tab. Values which control their operation are configured
      over under the Settings tab.</para>

      <sect3 id="urlcanon">
        <title>URL Canonicalization Rules</title>

        <para>Heritrix keeps a list of already seen URLs and before fetching,
        does a look up into this 'already seen' or 'already included' list to
        see if the URL has already been crawled. Often an URL can be written
        in multiple ways but the page fetched is the same in each case. For
        example, the page that is at
        <literal>http://www.archive.org/index.html</literal> is the same page
        as is at <literal>http//WWW.ARCHIVE.ORG/</literal> though the URLs
        differ (In this case by case only). Before going to the 'already
        included' list, Heritrix makes an effort at equating the likes of
        <literal>http://www.archive.org/index.html</literal> and
        <literal>http://ARCHIVE.ORG/</literal> by running each URL through a
        set of canonicalization rules. Heritrix uses the result of this
        canonicalization process when it goes to test if an URL has already
        been seen.</para>

        <para>An example of a canonicalization rule would lowercase all URLs.
        Another might strip the 'www' prefix from domains.</para>

        <para>The <literal>URL Canonicalization Rules</literal> screen allows
        you to specify canonicalization rules and the order in which they are
        run. A default set lowercases, strips wwws, removes sessionids and
        does other types of fixup such as removal of any userinfo. The URL
        page works in the same manner as the <xref linkend="filters" />
        page.</para>

        <para>To watch the canonicalization process, enable
        <literal>org.archive.crawler.url.Canonicalizer</literal> logging in
        <literal>heritrix.properties</literal> (There should already be a
        commented out directive in the properties file. Search for it). Output
        will show in <literal>heritrix_out.log</literal>. Set the logging
        level to INFO to see just before and after the transform. Set level to
        FINE to see the result of each rule's transform.</para>

        <para>Canonicalization rules can be added as an override so an added
        rule only works in the overridden domain.</para>

        <para>Canonicalization rules are NOT run if the URI-to-check is the
        fruit of a redirect. We do this for the following reason. Lets say the
        www canonicalization rule is in place (the rule that equates
        'archive.org' and 'www.archive.org'). If the crawler first encounters
        'archive.org' but the server at archive.org wants us to come in via
        'www.archive.org', it will redirect us to 'www.archive.org'. The
        alreadyseen database will have been marked with 'archive.org' on the
        original access of 'archive.org'. The www canonicalization rule runs
        and makes 'archive.org' of 'www.archive.org' which has already been
        seen. If we always ran canonicalization rules regardless, we wouldn't
        ever crawl 'www.archive.org'.</para>

        <sect4 id="urlcanonexample">
          <title>URL Canonicalization Use Case: Stripping Site-Particular
          Session IDs</title>

          <para>Say site x.y.z is returning URLs with a session ID key of
          <literal>cid</literal> as in
          <literal>http://x.y.z/index.html?cid=XYZ123112232112229BCDEFFA0000111</literal>.
          Say the session ID value is always 32 characters. Say also, for
          simplicity's sake, that it always appears on the end of the
          URL.</para>

          <sect5 id="urlcanonexamplesoln">
            <title>Solution</title>

            <para>Add a RegexRule override for the domain x.y.z. To do this,
            pause the crawl, add an override for x.y.z by clicking on the
            <literal>overrides</literal> tab in the main menu bar and filling
            in the domain x.y.z. Once in the override screen, click on the
            <literal>URL</literal> tab in the override menu bar -- the new bar
            that appears below the main bar when in override mode -- and add a
            <literal>RegexRule</literal> canonicalization rule. Name it
            <literal>cidStripper</literal>. Adjust where you'd like it to
            appear in the running of canonicalization rules (Towards the end
            should be fine). Now browse back to the override settings. The new
            canonicalization rule <literal>cidStripper</literal> should appear
            in the settings page list of canonicalization rules. Fill in the
            RegexRule <literal>matching-regex</literal> with something like
            the following: <literal>^(.+)(?:cid=[0-9a-zA-Z]{32})?$</literal>
            (Match a tail of 'cid=SOME_32_CHAR_STR' grouping all that comes
            before this tail). Fill into the <literal>format</literal> field
            <literal>${1}</literal> (This will copy the first group from the
            regex if the regex matched). To see the rule in operation, set the
            logging level for
            <literal>org.archive.crawler.url.Canonicalizer</literal> in
            <literal>heritrix.properties</literal> (Try uncommenting the line
            <literal>org.archive.crawler.url.Canonicalizer.level =
            INFO</literal>). Study the output and adjust your regex
            accordingly.</para>

            <para>See also <ulink
            url="http://groups.yahoo.com/group/archive-crawler/message/1611">msg1611</ulink>
            for another's experience getting regex to work.</para>
          </sect5>
        </sect4>
      </sect3>

      <sect3 id="filters">
        <title>Filters</title>

        <para>Filters are modules that take a <xref linkend="crawluri" /> and
        determine if it matches the criteria of the filter. If so it returns
        true, otherwise it returns false.</para>

        <para>Filters are used in a couple of different contexts in
        Heritrix.</para>

        <para>Their use in scopes has already been discussed in <xref
        linkend="scopes" /> and the problems with using them that in <xref
        linkend="scopeproblems" />.</para>

        <note>
          <para>A DecidingFilter was added in 1.4.0 to address problems with
          current filter model. DecideRules can be added into a DecidingFilter
          with the filter decision the result of all included DecideRule set
          processing. There are DecideRule equivalents for all Filter-types
          mentioned below. See <xref linkend="decidingscope" /> for more on
          the particulars of DecideRules and on the new Deciding model in
          general.</para>
        </note>

        <para>Aside from scopes, filters are also used in processors. Filters
        applied to processors always filter URIs <emphasis>out</emphasis>.
        That is to say that any URI matching a filter on a processor will
        effectively skip over that processor.</para>

        <para>This can be useful to disable (for instance) link extraction on
        documents coming from a specific section of a given website.</para>

        <sect4>
          <title>Adding, removing and reordering filters</title>

          <para>The Submodules page of the configuration section of the WUI
          lists existing filters along with the option to remove, add, or move
          Filters up or down in the listing.</para>

          <para>Adding a new filters requires giving it a unique name (for
          that list), selecting the class type of the filter from a combobox
          and clicking the associated add button. After the filter is added,
          its custom settings, if any, will appear in the Settings page of the
          configuration UI.</para>

          <para>Since filters can in turn contain other filters (the OrFilter
          being the best example of this) these lists can become quite complex
          and at times confusing.</para>
        </sect4>

        <sect4>
          <title>Provided filters</title>

          <para>The following is an overview of the most useful of the filters
          provided with Heritrix.</para>

          <sect5>
            <title>org.archive.crawler.filter.OrFilter</title>

            <para>Contains any number of filters and returns true if any of
            them returns true. A logical OR on its filters basically.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.URIRegExpFilter</title>

            <para>Returns true if a URI matches the regular expression set for
            it. See <xref linkend="regexpr" /> for more about regular
            expressions in Heritrix.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.ContentTypeRegExpFilter</title>

            <para>This filter runs a regular expression against the response
            <literal>Content-Type</literal> header. Returns true if content
            type matches the regular expression. ContentType regexp filter
            cannot be used until after fetcher processors have run. Only then
            is the Content-Type of the response known. A good place for this
            filter is the writer step in processing. See <xref
            linkend="regexpr" /> for more about regular expressions in
            Heritrix.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.SurtPrefixFilter</title>

            <para>Returns true if a URI is prefixed by one of the <xref
            linkend="surtprefix" />es supplied by an external file.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.FilePatternFilter</title>

            <para>Compares suffix of a passed URI against a regular expression
            pattern, returns true for matches.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.PathDepthFilter</title>

            <para>Returns true for all <xref linkend="crawluri" /> passed in
            with a path depth less or equal to its
            <literal>max-path-depth</literal> value.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.PathologicalPathFilter</title>

            <para>Checks if a URI contains a repeated pattern.</para>

            <para>This filter checks if a pattern is repeated a specific
            number of times. The use is to avoid crawler traps where the
            server adds the same pattern to the requested URI like:</para>

            <para><programlisting>  http://host/img/img/img/img....</programlisting></para>

            <para>Returns true if such a pattern is found. Sometimes used on a
            processor but is primarily of use in the exclude section of
            scopes.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.HopsFilter</title>

            <para>Returns true for all URIs passed in with a <xref
            linkend="link-hop-count" /> greater than the
            <literal>max-link-hops</literal> value.</para>

            <para>Generally only used in scopes.</para>
          </sect5>

          <sect5>
            <title>org.archive.crawler.filter.TransclusionFilter</title>

            <para>Filter which returns true for <xref linkend="crawluri" />
            instances which contain more than zero but fewer than
            <literal>max-trans-hops</literal> embed entries at the end of
            their <xref linkend="discoverypath" />.</para>

            <para>Generally only used in scopes.</para>
          </sect5>
        </sect4>
      </sect3>

      <sect3 id="credentials">
        <title>Credentials</title>

        <para>In this section you can add login credentials that will allow
        Heritrix to gain access to areas of websites requiring authentication.
        As with all modules they are only added here (supplying a unique name
        for each credential) and then configured on the settings page (<xref
        linkend="settings" />).</para>

        <para>One of the settings for each credential is its
        <literal>credential-domain</literal> and thus it is possible to create
        all credentials on the global level. However since this can cause
        excessive unneeded checking of credentials it is recommended that
        credentials be added to the appropriate domain override (see <xref
        linkend="overrides" /> for details). That way the credential is only
        checked when the relevant domain is being crawled.</para>

        <para>Heritrix can do two types of authentication: <ulink
        url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC and
        DIGEST Auth) and POST and GET of an HTML Form.</para>

        <note>
          <title>Logging</title>

          <para>To enable text console logging of authentication interactions
          (for example for debugging), set the FetchHTTP and
          PrconditionEnforcer log levels to fine</para>

          <para><programlisting>org.archive.crawler.fetcher.FetchHTTP.level = FINE
org.archive.crawler.prefetch.PreconditionEnforcer.level = FINE</programlisting></para>

          <para>This is done by editing the
          <filename>heritrix.properties</filename> file under the
          <filename>conf</filename> directory as described in <xref
          linkend="heritrix.properties" />.</para>
        </note>

        <sect4>
          <title><ulink
          url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC
          and DIGEST Auth)</title>

          <para>Supply <ulink url="#cd">credential-domain</ulink>, <ulink
          url="realm">realm</ulink>, login, and password.</para>

          <para>The way that the RFC2617 authentication works in Heritrix is
          that in response to a 401 response code (Unauthorized), Heritrix
          will use a key made up of the Credential Domain plus Realm to do a
          lookup into its Credential Store. If a match is found, then the
          credential is loaded into the CrawlURI and the CrawlURI is marked
          for immediate retry.</para>

          <para>When the requeued CrawlURI comes around again, this time
          through, the found credentials are added to the request. If the
          request succeeds -- result code of 200 -- the credentials are
          promoted to the CrawlServer and all subsequent requests made against
          this CrawlServer will preemptively volunteer the credential. If the
          credential fails -- we get another 401 -- then the URI is let die a
          natural 401 death.</para>

          <sect5 id="cd">
            <title>credential-domain</title>

            <para>This equates to the canonical root URI of RFC2617;
            effectively, in our case, its the CrawlServer name or <ulink
            url="http://java.sun.com/j2se/1.4.2/docs/api/java/net/URI.html">URI
            authority</ulink> (domain plus port if other than port 80).
            Examples of credential-domain would be: 'www.archive.org' or
            'www.archive.org:8080', etc.</para>
          </sect5>

          <sect5 id="realm">
            <title>realm</title>

            <para>Realm as per <ulink
            url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink>. The
            realm string must match exactly the realm name presented in the
            authentication challenge served up by the web server</para>
          </sect5>

          <sect5>
            <title>Known Limitations</title>

            <simplesect>
              <title>One Realm per Credential Domain Only</title>

              <para>Currently, you can only have one realm per credential
              domain.</para>
            </simplesect>

            <simplesect>
              <title>Digest Auth works for Apache</title>

              <para>... but your mileage may vary going up against other
              servers (See <ulink
              url="http://sourceforge.net/tracker/index.php?func=detail&amp;aid=914301&amp;group_id=73833&amp;atid=539102">[
              914301 ] Logging in (HTTP POST, Basic Auth, etc.)</ulink> to
              learn more).</para>
            </simplesect>
          </sect5>
        </sect4>

        <sect4>
          <title>HTML Form POST or GET</title>

          <para>Supply <ulink url="#cdh">credential-domain</ulink>, <ulink
          url="httpmethod">http-method</ulink>, <ulink
          url="loginuri">login-uri</ulink>, and <ulink
          url="formitems">form-items</ulink>, .</para>

          <para>Before a <literal>uri</literal> is scheduled, we look for
          preconditions. Examples of preconditions are the getting of the the
          dns record for the server that hosts the <literal>uri</literal> and
          the fetching of the <literal>robots.txt</literal>: i.e. we don't
          fetch any <literal>uri</literal> unless we first have gotten the
          <literal>robots.txt</literal> file. The HTML Form Credentials are
          done as a precondition. If there are HTML Form Credentials for a
          particular crawlserver in the credential store, the uri specified in
          the HTML Form Credential login-uri field is scheduled as a
          precondition for the site, after the fetching of the dns and robots
          preconditions.</para>

          <sect5 id="cdh">
            <title>credential-domain</title>

            <para>Same as the Rfc22617 Credential <ulink
            url="#cd">credential-domain</ulink>.</para>
          </sect5>

          <sect5 id="loginuri">
            <title>login-url</title>

            <para>Relative or absolute URI to the page that the HTML Form
            submits to (Not the page that contains the HTML Form).</para>
          </sect5>

          <sect5 id="formitems">
            <title>form-items</title>

            <para>Listing of HTML Form key/value pairs. Don't forget to
            include the form submit button.</para>
          </sect5>

          <sect5>
            <title>Known Limitations</title>

            <simplesect>
              <title>Site is crawled logged in or not; cannot do both</title>

              <para>If a site has an HTML Form Credential associated, the next
              thing done after the getting of the dns record and the
              robots.txt is that a login is performed against all listed HTML
              Form Credential login-uris. This means that the crawler will
              only ever view sites that have HTML Form Credentials from the
              'logged-in' perspective. There is no way currently of telling
              the crawler to crawl the site 'non-logged-in' and then, when
              done, log in and crawl the site anew only this time from the
              'logged-in' perspective (At least, not as part of the one crawl
              job).</para>
            </simplesect>

            <simplesect>
              <title>No means of verifying or rerunning login</title>

              <para>The login is run once only and the crawler continues
              whether the login succeeded or not. There is no means of telling
              the crawler retry upon unsuccessful authentication. Neither is
              there a means for the crawler to report success or otherwise
              (The operator is expected to study logs to see whether
              authentication ran successfully).</para>
            </simplesect>
          </sect5>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="settings">
      <title>Settings</title>

      <para>This page presents a semi-treelike representation of all the
      modules (fixed and pluggable alike) that make up the current
      configuration and allows the user to edit any of their settings. Go to
      the Modules and SubModules tabs to add, remove, replace modules
      mentioned here in the Settings page.</para>

      <para>The first option presented directly under the top tabs is whether
      to hide or display 'expert settings'. Expert settings are those settings
      that are rarely changed and should only be changed by someone with a
      clear understanding of their implication. This document will not discuss
      any of the expert settings.</para>

      <para>The first setting is the description of the job previously
      discussed. The seed list is at the bottom of the page. Between the two
      are all the other possible settings.</para>

      <para>Module names are presented in bold and a short explanation of them
      is provided. As discussed in the previous three chapters some of them
      can be replaced, removed or augmented.</para>

      <para>Behind each module and settings name a small question mark is
      present. By clicking on it a more detailed explanation of the relevant
      item pops up. For most settings users should refer to that as their
      primary source of information.</para>

      <para>Some settings provide a fixed number of possible 'legal' values in
      combo boxes. Most are however typical text input fields. Two types of
      settings require a bit of additional attention.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Lists</emphasis></para>

          <para>Some settings are a list of values. In those cases a list is
          printed with an associated <emphasis>Remove</emphasis> button and an
          input box is printed below it with an <emphasis>Add</emphasis>
          button. Only those items in the list box are considered in the list
          itself. A value in the input box does not become a part of the list
          until the user clicks <emphasis>Add</emphasis>. There is no way to
          edit existing values beyond removing them and replacing them with
          correct values. It is also not possible to reorder the list.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Simple typed maps</emphasis></para>

          <para>Generally Maps in the Heritrix settings framework contain
          program modules (such as the processors for example) and are
          therefore edited elsewhere. However maps that only accept simple
          data types (Java primitives) can be edited here.</para>

          <para>They are treated as a key, value pair. Two input boxes are
          provided for new entries with the first one representing the key and
          the second the value. Clicking the associated
          <emphasis>Add</emphasis> button adds the entry to the map. Above the
          input boxes a list of existing entries is displayed along with a
          <emphasis>Remove</emphasis> option. Simple maps can not be
          reordered.</para>
        </listitem>
      </itemizedlist>

      <para>Changes on this page are not saved until you navigate to another
      part of the settings framework or you click the submit job/finished
      tab.</para>

      <para>If there is a problem with one of the settings a red star will
      appear next to it. Clicking the star will display the relevant error
      message.</para>

      <sect3>
        <title>Basic settings</title>

        <para>Some settings are always present. They form the so called crawl
        order. The root of the settings hierarchy that other modules plug
        into.</para>

        <sect4>
          <title>Crawl limits</title>

          <para>In addition to limits imposed on the scope of the crawl it is
          possible to enforce arbitrary limits on the duration and extent of
          the crawl with the following settings:</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">max-bytes-download</emphasis></para>

              <para>Stop after a fixed number of bytes have been downloaded. 0
              means unlimited.</para>
            </listitem>

            <listitem>
              <para><emphasis
              role="bold">max-document-download</emphasis></para>

              <para>Stop after downloading a fixed number of documents. 0
              means unlimited.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">max-time-sec</emphasis></para>

              <para>Stop after a certain number of seconds have elapsed. 0
              means unlimited.</para>

              <para>For handy reference there are 3600 seconds in an hour and
              86400 seconds in a day.</para>
            </listitem>
          </itemizedlist>

          <note>
            <para>These are not hard limits. Once one of these limits is hit
            it will trigger a graceful termination of the crawl job, that
            means that URIs already being crawled will be completed. As a
            result the set limit will be exceeded by some amount.</para>
          </note>
        </sect4>

        <sect4>
          <title>max-toe-threads</title>

          <para>Set the number of toe threads (see <xref
          linkend="toethreads" />).</para>

          <para>If running a domain crawl smaller than 100 hosts a value
          approximately twice the number of hosts should be enough. Values
          larger then 150-200 are rarely worthwhile unless running on machines
          with exceptional resources.</para>
        </sect4>

        <sect4 id="httpheaders">
          <title>HTTP headers</title>

          <para>Currently Heritrix supports configuring the
          <literal>user-agent</literal> and <literal>from</literal> fields in
          the HTTP headers generated when requesting URIs from
          webservers.</para>

          <sect5 id="from">
            <title>from</title>

            <para>The <literal>from</literal> attribute must contain a valid
            e-mail address.</para>
          </sect5>

          <sect5 id="user-agent">
            <title>user-agent</title>

            <para>The initial user-agent template you see when you first start
            heritrix will look something like the following:</para>

            <para><programlisting>Mozilla/5.0 (compatible; heritrix/0.11.0 +PROJECT_URL_HERE</programlisting></para>

            <para>You must change at least the
            <literal>PROJECT_URL_HERE</literal> and put in place a website
            that webmasters can go to to view information on the organization
            or person running a crawl.</para>

            <para>The <literal>user-agent</literal> string must adhere to the
            following format:</para>

            <para><programlisting>[optional-text] ([optional-text] +PROJECT_URL [optional-text]) [optional-text]</programlisting></para>

            <para>The parenthesis and plus sign before the URL must be
            present. Other examples of valid user agents would include:</para>

            <para><programlisting>my-heritrix-crawler (+http://mywebsite.com)
Mozilla/5.0 (compatible; bush-crawler +http://whitehouse.gov)
Mozilla/5.0 (compatible; os-heritrix/0.11.0 +http://loc.gov on behalf to the Library of Congress)</programlisting></para>
          </sect5>
        </sect4>

        <sect4>
          <title>Robots honoring policy</title>

          <para>There are five types of policies offered on how to deal with
          <literal>robots.txt</literal> rules:</para>

          <orderedlist>
            <listitem>
              <para><emphasis role="bold">classic</emphasis></para>

              <para>Simply obey the <literal>robots.txt</literal> rules.
              Recommended unless you have special permission to collect a site
              more aggressively.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">ignore</emphasis></para>

              <para>Completely ignore <literal>robots.txt</literal>
              rules.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">custom</emphasis></para>

              <para>Obey user set, custom, <literal>robots.txt</literal> rules
              instead of those discovered on the relevant site.</para>

              <para>Mostly useful in overrides.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">most-favored</emphasis></para>

              <para>Obey the rules set in the <literal>robots.txt</literal>
              for the robot that is allowed to access the most or has the
              least restrictions. Can optionally masquerade as said
              robot.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">most-favored-set</emphasis></para>

              <para>Same as 4, but limit the robots whose rules we can follow
              to a given set.</para>
            </listitem>
          </orderedlist>

          <note>
            <para>Choosing options 3-5 requires setting additional information
            in the fields below the policy combobox. For options 1 and 2 those
            can be ignored.</para>
          </note>
        </sect4>
      </sect3>

      <sect3>
        <title>Scope settings</title>

        <para>The different scopes do share a few common settings. (See <xref
        linkend="scopes" /> for more on scopes provided with Heritrix.)</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">max-link-hops</emphasis></para>

            <para>Maximum number of links followed to get to the current URI.
            Basically counts 'L's in the <xref
            linkend="discoverypath" />.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">max-trans-hops</emphasis></para>

            <para>Maximum number of non link hops at the end of the current
            URI's <xref linkend="discoverypath" />. Generally we don't want to
            follow embeds, redirects and the like for more than a few (default
            5) hops in a row. Such deep embedded structures are usually
            crawler traps. Since embeds are usually treated with higher
            priority then links, getting stuck in this type of trap can be
            particularly harmful to the crawl.</para>
          </listitem>
        </itemizedlist>

        <para>Additionally scopes may possess many more settings, depending on
        what filters are attached to them. See the related pop-up help in the
        WUI for information on those.</para>
      </sect3>

      <sect3>
        <title>Frontier settings</title>

        <para>The Frontier provided with Heritrix has a few settings of
        particular interest.</para>

        <sect4>
          <title>Politeness</title>

          <para>A combination of four settings controls the politeness of the
          Frontier. Before we cover them it is important to note that at any
          given time only one URI from any given <xref linkend="host" /> is
          being processed. The following politeness rules all revolve around
          imposing additional wait time between the end of processing one URI
          and until the next one starts.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">delay-factor</emphasis></para>

              <para>Imposes a delay between URIs from the same host that is a
              multiple of the amount of time it took to fetch the last URI
              downloaded from that host.</para>

              <para>For example if it took 800 milliseconds to fetch the last
              URI from a host and the delay-factor is 5 (a very high value)
              then the Frontier will wait 4000 milliseconds (4 seconds) before
              allowing another URI from that host to be processed.</para>

              <para>This value can be set to 0 for maximum impoliteness. It is
              never possible to have multiple concurrent URIs being processed
              from the same host.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">max-delay-ms</emphasis></para>

              <para>This setting allows the user to set a maximum upper limit
              on the 'in between URIs' wait created by the delay factor. If
              set to 1000 milliseconds then in the example used above the
              Frontier would only hold URIs from that host for 1 second
              instead of 4 since the delay factor exceeded this ceiling
              value.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">min-delay-ms</emphasis></para>

              <para>Similar to the maximum limit, this imposes a minimum limit
              to the politeness. This can be useful to ensure, for example,
              that at least 100 milliseconds elapse between connections to the
              same host. In a case where the delay factor is 2 and it only
              took 20 milliseconds to get a URI this would come into
              effect.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">min-interval-ms</emphasis></para>

              <para>An alternate way of putting a floor on the delay, this
              specifies the minimum number of milliseconds that must elapse
              from the <emphasis>start of processing </emphasis>one URI until
              the next one after it starts. This can be useful in cases where
              sites have a mix of large files that take an excessive amount of
              time and very small files that take virtually no time.</para>

              <para>In all cases (this can vary from URI to URI) the more
              restrictive (delaying) of the two floor values is
              imposed.</para>
            </listitem>
          </itemizedlist>
        </sect4>

        <sect4>
          <title>Retry policy</title>

          <para>The Frontier imposes a policy on retrying URIs that
          encountered errors that usually are transitory (socket timeouts
          etc.) . Fetcher processors may also have their own policies on
          certain aspects of this.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">max-retries</emphasis></para>

              <para>How often to retry URIs that encounter possible transient
              errors.</para>
            </listitem>

            <listitem>
              <para><emphasis
              role="bold">retry-delay-seconds</emphasis></para>

              <para>How long to wait between such retries.</para>
            </listitem>
          </itemizedlist>
        </sect4>

        <sect4>
          <title>Bandwidth limits</title>

          <para>The Frontier allows the user to limit bandwidth usage. This is
          done by holding back URIs when bandwidth usage has exceeded limits.
          As a result individual spikes of bandwidth usage can occur that
          greatly exceed this limit. This only limits overall bandwidth usage
          over a longer period of time (minutes).</para>

          <itemizedlist>
            <listitem>
              <para><emphasis
              role="bold">total-bandwidth-usage-KB-sec</emphasis></para>

              <para>Maximum bandwidth to use in Kilobytes per second.</para>
            </listitem>

            <listitem>
              <para><emphasis
              role="bold">max-per-host-bandwidth-usage-KB-sec</emphasis></para>

              <para>Maximum bandwidth to use in dealing with any given host.
              This is a form of politeness control as it limits the load
              Heritrix places on a host.</para>
            </listitem>
          </itemizedlist>
        </sect4>
      </sect3>

      <sect3>
        <title>Processors settings</title>

        <para>A couple of the provided processors have settings that merit
        some extra attention.</para>

        <para>As has been noted elsewhere each processor has a setting named
        <emphasis role="bold">enabled</emphasis>. This is set to true by
        default, but can be set to false to effectively remove it from
        consideration. Processors whose enabled setting is set to false will
        not be applied to any applicable URI (this is of greatest use in
        overrides and refinements).</para>

        <sect4>
          <title>HTTP Fetcher</title>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">timeout-seconds</emphasis></para>

              <para>If a fetch is not completed within this many seconds, the
              HTTP fetcher will terminate it.</para>

              <para>Should generally be set quite high.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">max-length-bytes</emphasis></para>

              <para>Maximum number of bytes to download per document. Will
              truncate file once this limit is reached.</para>

              <para>By default this value is set to an extremely large value
              (in the exabyte range) that will never be reached in
              practice.</para>
            </listitem>
          </itemizedlist>

          <para id="midfetch">Its also possible to add in filters that are
          checked after the download of the HTTP response headers but before
          the response content is read. Use
          <literal>midfetch-filters</literal> to abort the download of
          content-types other than those wanted (Aborted fetches have an
          annotation <literal>midFetchAbort</literal> appended to the
          <literal>crawl.log</literal> entry). Note that unless the same
          filters are applied at the writer processing step, the response
          headers -- but not the content -- will show in ARC files.</para>
        </sect4>

        <sect4>
          <title>Archiver</title>

          <para>The ARC writer processor can be configured somewhat. This
          mostly relates to how the ARC files are written to disk.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">compress</emphasis></para>

              <para>Write compressed ARC files true or false.<note>
                  <para>Each item that is added to the ARC file will be
                  compressed individually.</para>
                </note></para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">prefix</emphasis></para>

              <para>A prefix to the ARC files filename. See <xref
              linkend="arcfiles" /> for more on ARC file naming.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">max-size-bytes</emphasis></para>

              <para>Maximum size per ARC file. Once this size is reached no
              more documents will be added to an ARC file, another will be
              created to continue the crawl. This is of course not a hard
              limit and the last item added to an ARC file will push its size
              above this limit. If exceptionally large items are being
              downloaded the size of an ARC file may exceed this value by a
              considerable amount since items will never be split between ARC
              files.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">path</emphasis></para>

              <para>Path where ARC files should be written. Can be a list of
              absolute paths. If relative paths, will be relative to the
              <literal>job</literal>directory. It can be safely configured
              mid-crawl to point elsewhere if current location is close to
              full. If multiple paths, then we'll choose from the list of
              paths in a round-robin fashion.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">pool-max-active</emphasis></para>

              <para>The Archiver maintains a pool of ARC files which are each
              ready to accept a downloaded documents, to prevent ARC writing
              from being a bottleneck in multithreaded operation. This setting
              establishes the maximum number of such files to keep ready.
              Default is 5. For small crawls that you want to confine to a
              single ARC file, this should be set to 1.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">pool-max-wait</emphasis></para>

              <para>The maximum amount of time to wait on the Archiver's pool
              element.</para>
            </listitem>
          </itemizedlist>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="overrides">
      <title>Overrides</title>

      <para>Overrides provide the ability to override individual settings on a
      per domain basis. The overrides page provides an iterative list of
      domains that contain override settings, that is values for parameters
      that override values in the global configuration.</para>

      <para>It is best to think of the general global settings as the root of
      the settings hierarchy and they are then overridden by top level domains
      (com, net, org, etc) who are in turn overridden by domains (yahoo.com,
      archive.org, etc.) who can further be overridden by subdomains
      (crawler.archive.org). There is no limit for how deep into the
      subdomains the overrides can go.</para>

      <para>When a URI is being processed the settings for its host is first
      looked up. If the needed setting is not available there, its super
      domains are checked until the setting is found (all settings exist at
      the global level at the very least).</para>

      <para>Creating a new override is done by simply typing in the domain in
      the input box at the bottom of the page and clicking the
      <emphasis>Create / Edit</emphasis> button. Alternatively if overrides
      already exist the user can navigate the hierarchy of existing overrides,
      edit them and create new overrides on domains that don't already have
      them.</para>

      <para>Once an override has been created or selected for editing the user
      is taken to a page that closely resembles the settings page discussed in
      <xref linkend="settings" />. The main difference is that those settings
      that can not be overridden (file locations, number of threads etc.) are
      printed in a non-editable manner. Those settings that can be edited now
      have a checkbox in front of them. If they are being overridden at the
      current level that checkbox should be checked. Editing a setting will
      cause the checkmark to appear. Removing the checkmark effectively
      removes the override on that setting.</para>

      <para>Once on the settings page the second level tabs will change to
      override context. The new tabs will be similar to the general tabs and
      will have:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">URL</emphasis></para>

          <para>Add URL Canonicalization Rules to the override. It is not
          possible to remove inherited filters or interject new filters among
          them. New filters will be added after existing filters.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Filters</emphasis></para>

          <para>Add filters to the override. It is not possible to remove
          inherited filters or interject new filters among them. New filters
          will be added after existing filters.</para>

          <para>Inherited filters though have the option to locally disable
          them. That can be set on the settings page.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Credentials</emphasis></para>

          <para>Add credentials to the override. Generally credentials should
          always be added to an override of the domain most relevant to them.
          See <xref linkend="credentials" /> for more details.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Settings</emphasis></para>

          <para>Page allowing the user to override specific settings as
          discussed above.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Refinements</emphasis></para>

          <para>Manage refinements for the override. See <xref
          linkend="refinements" /></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Done with override</emphasis></para>

          <para>Once the user has finished with the override, this option will
          take him back to the overrides overview page.</para>
        </listitem>
      </itemizedlist>

      <para>It is not possible to add, remove or reorder existing modules on
      an override. It is only possible to add filters and credentials. Those
      added will be inherited to sub domains of the current override domain.
      Those modules that are added in an override will not have a checkbox in
      front of their settings on the override settings page since the override
      is effectively their 'root'.</para>

      <para>Finally, due to how the settings framework is structured there is
      negligible performance penalty to using overrides. Lookups for settings
      take as much time whether or not overrides have been defined. For URIs
      belonging to domains without overrides no performance penalty is
      incurred.</para>
    </sect2>

    <sect2 id="refinements">
      <title>Refinements</title>

      <para>Refinements are similar to overrides (see <xref
      linkend="overrides" />) in that they allow the user to modify the
      settings under certain circumstances. There are however two major
      differences.</para>

      <orderedlist>
        <listitem>
          <para>Refinements are applied based on arbitrary criteria rather
          then encountered URIs domain.</para>

          <para>Currently it is possible to set criteria based on the time of
          day, a regular expression matching the URI and the port number of
          the URI.</para>
        </listitem>

        <listitem>
          <para>They incur a performance penalty.</para>

          <para>This effect is small if their numbers are few but for each URI
          encountered there must be a check made to see if it matches any of
          the existing criteria of defined refinements.</para>

          <para>This effect can be mitigated by applying refinements to
          overrides rather then the global settings.</para>
        </listitem>
      </orderedlist>

      <para>Refinements can be applied either to the global settings or to any
      override. If applied to an override they can affect any settings,
      regardless of whether the parent override has modified it.</para>

      <para>It is not possible to create refinements on refinements.</para>

      <para>Clicking the <emphasis>Refinements</emphasis> tab on either the
      global settings or an override brings the user to the refinements
      overview page. The overview page displays a list of existing refinements
      on the current level and allows the user to create new ones.</para>

      <para>To create a new refinement the user must supply a unique name for
      it (name is limited to letters, numbers, dash and underscore) and a
      short description that will be displayed underneath it on the overview
      page.</para>

      <para>Once created, refinements can be either removed or edited.</para>

      <para>Choosing the edit option on an override brings the user to the
      criteria page. Aside from the criteria tab replacing the refinements
      tab, the second level tabs will have the same options as they do for
      overrides and their behavior will be the same. Clicking the 'Done with
      refinement' tab will bring the user back to the refinements overview
      page.</para>

      <sect3>
        <title>Criteria</title>

        <para>The criteria page displays a list of the current criteria and
        the option to add any of the available criteria types to the list. It
        is also possible to remove existing criteria.</para>

        <note>
          <para>URIs must match <emphasis role="bold">all</emphasis> set
          criteria for the refinement to take effect.</para>
        </note>

        <para>Currently the following criteria can be applied:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Port number</emphasis></para>

            <para>Match only those URIs for the given port number.</para>

            <para>Default port number for HTTP is 80 and 443 for HTTPS.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Time of day</emphasis></para>

            <para>If this criteria is applied the refinement will be in effect
            between the hours specified each day.</para>

            <para>The format for the input boxes is HHMM (hours and
            minutes).</para>

            <para>An example might be: From 0200, To 0600. This refinement
            would be in effect between 2 and 6 am each night. Possibly to ease
            the politeness requirements during these hours when load on
            websites is generally low.<note>
                <para>As with all times in Heritrix these are <emphasis
                role="bold">always GMT</emphasis> times.</para>
              </note></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Regular expression</emphasis></para>

            <para>The refinement will only be in effect for those URIs that
            match the given regular expression.<note>
                <para>See <xref linkend="regexpr" /> for more on them.</para>
              </note></para>
          </listitem>
        </itemizedlist>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="running">
    <title>Running a job</title>

    <para>Once a crawl job has been created and properly configured it can be
    run. To start a crawl the user must go to the web Console page (via the
    Console tab).</para>

    <sect2 id="console">
      <title>Web Console</title>

      <para>The web Console presents on overview of the current status of the
      crawler.</para>

      <sect3>
        <title>Crawler Status Box</title>

        <para>The following information is always provided:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Crawler Status</emphasis></para>

            <para>Is the crawler in <emphasis>Holding Jobs</emphasis> or
            <emphasis>Crawling Jobs</emphasis> mode? If holding, no new jobs
            pending or created will be started (but a job already begun will
            continue). If crawling, the next pending or created job will be
            started as soon as possible, for example when a previous job
            finishes. For more detail see <xref
            linkend="holdingvcrawling" />.</para>

            <para>To the right of the current crawler status, a control link
            reading either "Start" or "Hold" will toggle the crawler between
            the two modes.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Jobs</emphasis></para>

            <para>If a current job is in progress, its status and name will
            appear. Alternatively, "None running" will appear to indicate no
            job is in progress because the crawler is holding, or "None
            available" if no job is in progress because no jobs have been
            queued.</para>

            <para>Below the current job info, the number of jobs pending and
            completed is shown. The completed count includes those that failed
            to start for some reason (see <xref linkend="failedtostart" /> for
            more on misconfigured jobs).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Alerts</emphasis></para>

            <para>Total number of alerts, and within brackets new alerts, if
            any.</para>

            <para>See <xref linkend="alerts" /> for more on alerts.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Memory</emphasis></para>

            <para>The amount of memory currently used, the size of the Java
            heap, and the maximum size to which the heap can possibly grow are
            all displayed, in kilobytes (KB).</para>
          </listitem>
        </itemizedlist>
      </sect3>

      <sect3>
        <title>Job Status Box</title>

        <para>If a job is in-progress -- running, paused, or between job
        states -- the following information is also provided in a second area
        underneath the <emphasis>Crawler Status Box</emphasis>.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Job Status</emphasis></para>

            <para>The current status of the job in progress. Jobs being
            crawled are usually running or paused.</para>

            <para>To the right of the current status, controls for
            pausing/resuming or terminating the current job will appear as
            appropriate.</para>

            <para>When a job is terminated, its status will be marked as
            'Ended by operator'. All currently active threads will be allowed
            to finish behind the scenes even though the WUI will report the
            job being terminated at once. If the crawler is in "Crawling Jobs"
            mode, a next pending job, if any, will start immediately.</para>

            <para>When a running job is paused, it may take some time for all
            the active threads to enter a paused state. Until then the job is
            considered to be still running and 'pausing'. It is possible to
            resume from this interim state.</para>

            <para>Once paused a job is considered to be suspended and time
            spent in that state does not count towards elapsed job time or
            rates.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Rates</emphasis></para>

            <para>The number of URIs successfully processed per second is
            shown, both the rate in the latest sampling interval and (in
            parentheses) the average rate since the crawl began. The sampling
            interval is typically about 20 seconds, and is adjustable via the
            "interval-seconds" setting. The latest rate of progress can
            fluctuate considerably, as the crawler workload varies and
            housekeeping memory and file operations occur -- especially if the
            sampling interval has been set to a low value.</para>

            <para>Also show is the rate of successful content collection, in
            KB/sec, for the latest sampling interval and (in parentheses) the
            average since the crawl began. (See <xref
            linkend="bytes" />.)</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Time</emphasis></para>

            <para>The amount of time that has elapsed since the crawl began
            (excluding any time spent paused) is displayed, as well as a very
            crude estimate of the require time remaining. (This estimate does
            not yet consider the typical certainty of discovering more URIs to
            crawl, and ignored other factors, so should not be relied upon
            until it can be improved in future releases.)</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Load</emphasis></para>

            <para>A number of measures are shown of how busy or loaded the job
            has made the crawler. The number of active threads, compared to
            the total available, is shown. Typically, if only a small number
            of threads are active, it is because activating more threads would
            exceed the configured politeness settings, given the remaining URI
            workload. (For example, if all remaining URIs are on a single
            host, no more than one thread will be active -- and often none
            will be, as polite delays are observed between requests.)</para>

            <para>The <emphasis>congestion ratio</emphasis> is a rough
            estimate of how much additional capacity, as a multiple of current
            capacity, would be necessary to crawl the current workload at the
            maximum rate allowable by politeness settings. (It is calculated
            by comparing the number of internal queues that are progressing
            with those that are waiting for a thread to become
            available.)</para>

            <para>The <emphasis>deepest queue</emphasis> number indicates the
            longest chain of pending URIs that must be processed sequentially,
            which is a better indicator of the work remaining than the total
            number of URIs pending. (A thousand URIs in a thousand independent
            queues can complete in parallel very quickly; a thousand in one
            queue will take longer.)</para>

            <para>The <emphasis>average depth</emphasis> number indicates the
            average depth of the last URI in every active sequential
            queue.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Totals</emphasis></para>

            <para>A progress bar indicates the relative percentage of
            completed URIs to those known and pending. As with the remaining
            time estimate, no consideration is given to the liklihood of
            discovering additional URIs to crawl. So, the percentage completed
            can shrink as well as grow, especially in broader crawls.</para>

            <para>To the left of the progress bar, the total number of URIs
            successfully downloaded is shown; to the right, the total number
            of URIs queued for future processing. Beneath the bar, the total
            of downloaded plus queued is shown, as well as the uncompressed
            total size of successfully downloaded data in kilobytes. See <xref
            linkend="bytes" />. (Compressed ARCs on disk will be somewhat
            smaller than this figure.)</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Paused Operations</emphasis></para>

            <para>When the job is paused, additional options will appear such
            as <emphasis>View or Edit Frontier URIs</emphasis>.</para>

            <para>The <emphasis>View or Edit Frontier URIs</emphasis> option
            takes the operator to a page allowing the lookup and deletion of
            URIs in the frontier by using a regular expression, or addition of
            URIs from an external file (even URIs that have already been
            processed).</para>
          </listitem>
        </itemizedlist>

        <para>Some of this information is replicated in the head of each page
        (see <xref linkend="header" />).</para>
      </sect3>

      <sect3>
        <title>Console Bottom Operations</title>

        <sect4>
          <title>Refresh</title>

          <para>Update the status display. The status display does not update
          itself and quickly becomes out of date as crawling proceeds. This
          also refreshes the options available if they've changed as a result
          of a change in the state of the job being crawled.</para>
        </sect4>

        <sect4>
          <title>Shut down Heritrix</title>

          <para>It is possible to shut down Heritrix through this option.
          Doing so will terminate the Java process running Heritrix and the
          only way to start it up again will be via the command line as this
          also disables the WUI.</para>

          <para>The user is asked to confirm this action twice to prevent
          accidental shut downs.</para>

          <para>This option will try to terminate any current job gracefully
          but will only wait a very short time for active threads to
          finish.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="pendingjobs">
      <title>Pending jobs</title>

      <para>At any given time there can be any number of crawl jobs waiting
      for their turn to be crawled.</para>

      <para>From the <emphasis>Jobs</emphasis> tab the user can access a list
      of these pending jobs (it also possible to get to them from the header,
      see <xref linkend="header" />).</para>

      <para>The list displays the name of each job, its status (currently all
      pending jobs have the status 'Pending') and offers the following options
      for each job:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">View order</emphasis></para>

          <para>Opens up the actual XML configuration file in a seperate
          window. Of interest to advanced users only.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Edit configuration</emphasis></para>

          <para>Takes the user to the Settings page of the jobs configurations
          (see <xref linkend="settings" />).</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Journal</emphasis></para>

          <para>Takes the user to the job's Journal (see <xref
          linkend="journal" />).</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Delete</emphasis></para>

          <para>Deletes the job (will only be marked as deleted, does not
          delete it from disk).</para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2>
      <title>Monitoring a running job</title>

      <para>In addition to the logs and reports generally available on all
      jobs (see <xref linkend="logs" /> and <xref linkend="reports" />) some
      information is provided only for jobs being crawled.<note>
          <para>The Crawl Report (see <xref linkend="crawlreport" />) contains
          one bit of information only available on active crawls. That is the
          amount of time that has elapsed since a URI belonging to each host
          was last finished.</para>
        </note></para>

      <sect3>
        <title>Internal reports on ongoing crawl</title>

        <para>The following reports are only availible while the crawler is
        running. They provide information about the internal status of certain
        parts of the crawler. Generally this information is only of interest
        to advanced users who possess detailed knowledge of the internal
        workings of said modules.</para>

        <para>These reports can be accessed from the Reports tab when a job is
        being crawled.</para>

        <sect4>
          <title>Frontier report</title>

          <para>A report on the internal state of the frontier.Can be unwieldy
          in size or the amount of time/memory it takes to compose in large
          crawls (with thousands of hosts with pending URIs).</para>
        </sect4>

        <sect4>
          <title>Thread report</title>

          <para>Contains information about what each toe thread is doing and
          how long it has been doing it. Also allows users to terminate
          threads that have become stuck. Terminated threads will not actually
          be removed from memory, Java does not provide a way of doing that.
          Instead they will be isolated from the rest of the program running
          and the URI they are working on will be reported back to the
          frontier as if it had failed to be processed.<caution>
              <para>Terminating threads should only be done by advanced users
              who understand the effect of doing so.</para>
            </caution></para>
        </sect4>

        <sect4 id="processorsreport">
          <title>Processors report</title>

          <para>A report on each processor. Not all processors provide
          reports. Typically these are numbers of URIs handled, links
          extracted etc.</para>

          <para>This report is saved to a file at the end of the crawl (see
          <xref linkend="processorsreport.txt" />).</para>
        </sect4>
      </sect3>

      <sect3 id="failedtostart">
        <title>Job failed to start</title>

        <para>If a job is misconfigured in such a way that it is not possible
        to do any crawling it might seem as if it never started. In fact what
        happens is that the crawl is started but on the initialization it is
        immediately terminated and sent to the list of completed jobs (<xref
        linkend="completedjobs" />). In those instances an explanation of what
        went wrong is displayed on the completed jobs page. An alert will also
        be created.</para>

        <para>A common cause of this is forgetting to set the HTTP header's
        <literal>user-agent</literal> and <literal>from</literal> attributes
        to valid values (see <xref linkend="httpheaders" />).</para>

        <para>If no processors are set on the job (or the modules otherwise
        badly misconfigured) the job may succeed in initializing but
        immediately exhaust the seed list, failing to actually download
        anything. This will not trigger any errors but a review of the logs
        for the job should highlight the problem. So if a job terminates
        immediately after starting without errors, the configuration
        (especially modules) should be reviewed for errors.</para>
      </sect3>

      <sect3 id="header">
        <title>All page status header</title>

        <para>At the top of every page in the WUI, right next to the Heritrix
        logo, is a brief overview of the crawler's current status.</para>

        <para>The three lines contain the following information (starting at
        the top left and working across and down).</para>

        <para>First bit of information is the current time when the page was
        displayed. This is useful since the status of the crawler will
        continue to change after a page loads, but those changes will not be
        reflected on the page until it is reloaded (usually manually by the
        user). As always this time is in GMT.</para>

        <para>Right next to it is the number of current and new alerts.</para>

        <para>The second line tells the user if the crawler is in "Crawling
        Jobs" or "Holding Jobs" mode. (See <xref
        linkend="holdingvcrawling" />). If a job is in progress, its status
        and name will also be shown.</para>

        <para>At the beginning of the final line the number of pending and
        completed jobs are displayed. Clicking on either value takes the user
        to the related overview page. Finally if a job is in progress, total
        current URIs completed, elapsed time, and URIs/sec figures are
        shown.</para>
      </sect3>

      <sect3>
        <title id="alerts">Alerts</title>

        <para>The number of existing and new alerts is displayed both in the
        Console (<xref linkend="console" />) and the header of each page
        (<xref linkend="header" />).</para>

        <para>Clicking on the link made up of those numbers takes the user to
        an overview of the alerts. The alerts are presented as messages, with
        unread ones clearly marked in bold and offering the user the option of
        reading them, marking as read and deleting them.</para>

        <para>Clicking an alert brings up a screen with its details.</para>

        <para>Alerts are generated in response to an error or problem of some
        form. Alerts have severity levels that mirror the Java log
        levels.</para>

        <para>Serious exception that occur will have a
        <emphasis>Severe</emphasis> level. These may be indicative of bugs in
        the code or problems with the configuration of a crawl job.</para>
      </sect3>
    </sect2>

    <sect2 id="editrun">
      <title>Editing a running job</title>

      <para>The configurations of a job can be edited while it is running.
      This option is accessed from the <emphasis>Jobs</emphasis> tab (Current
      job/Edit configuration). When selected the user is taken to the settings
      section of the job's configuration (see s<xref
      linkend="settings" />).</para>

      <para>When a configuration file is edited, the old version of it is
      saved to a new file (new file is named
      <filename>&lt;oldFilename&gt;_&lt;timestamp&gt;.xml</filename>) before
      it is updated. This way a record is kept of any changes. This record is
      only kept for changes made <emphasis>after</emphasis> crawling
      begins.</para>

      <para>It is not possible to edit all aspects of the configuration after
      crawling starts. Most noticably the Modules section is disabled. Also,
      although not enforced by the WUI, making changes to certain settings (in
      particular filenames, directory locations etc.) will have no effect
      (doing so will typically not harm the crawl, it will simply be
      ignored).</para>

      <para>However most settings can be changed. This includes the number of
      threads being used and the seeds list and although it is not possible to
      remove modules, most have the option to disable them. Settings a modules
      <literal>enabled</literal> attribute to <literal>false</literal>
      effectively removes them from the configuration.</para>

      <para>If changing more than an existing atomic value -- for example,
      adding a new filter -- it is good practice to pause the crawl first, as
      some modifications to composite configuration entities may not occur in
      a thread-safe manner with respect to ongoing crawling otherwise.</para>

      <sect3>
        <title id="journal">Journal</title>

        <para>The user can add notes to a journal that is kept for each job.
        No entries are made automatically in the journal, it is only for user
        added comments.</para>

        <para>It can be useful to use it to document reasons behind
        configuration changes to preserve that information along with the
        actual changes.</para>

        <para>The journal can be accessed from the Pending jobs page (<xref
        linkend="pendingjobs" />) for pending jobs, the Jobs tab for currently
        running jobs and the Completed jobs page (<xref
        linkend="completedjobs" />) for completed jobs.</para>

        <para>The journal is written to a plain text file that is stored along
        with the logs.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="analysis">
    <title>Analysis of jobs</title>

    <para>Heritrix offers several facilities for examining the details of a
    crawl. The reports and logs are also availible at run time.</para>

    <sect2 id="completedjobs">
      <title>Completed jobs</title>

      <para>In the <emphasis>Jobs</emphasis> tab (and page headers) is a
      listing of how many completed jobs there are along with a link to a page
      that lists them.</para>

      <para>The following information / options are provided for each
      completed job:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">UID</emphasis></para>

          <para>Each job has a unique (generated) ID. This is actually a time
          stamp. It differentiates jobs with the same name from one
          another.</para>

          <para>This ID is used (among other things) for creating the job's
          directory on disk.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Job name</emphasis></para>

          <para>The name that the user gave the job.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Status</emphasis></para>

          <para>Status of the job. Indicates how it ended.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Options</emphasis></para>

          <para>In addtion the following options are available for each
          job.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis>Crawl order</emphasis></para>

              <para>Opens up the actual XML file of the jobs configuration in
              a seperate window. Generally only of interest to advanced
              users.</para>
            </listitem>

            <listitem>
              <para><emphasis>Crawl report</emphasis></para>

              <para>Takes the user to the job's Crawl report (<xref
              linkend="crawlreport" />).</para>
            </listitem>

            <listitem>
              <para><emphasis>Seeds report</emphasis></para>

              <para>Takes the user to the job's Seeds report (<xref
              linkend="seedsreport" />).</para>
            </listitem>

            <listitem>
              <para><emphasis>Seed file</emphasis></para>

              <para>Displays the seed</para>
            </listitem>

            <listitem>
              <para><emphasis>Logs</emphasis></para>

              <para>Takes the user to the job's logs (<xref
              linkend="logs" />).</para>
            </listitem>

            <listitem>
              <para><emphasis>Journal</emphasis></para>

              <para>Takes the user to the Journal page for the job (<xref
              linkend="journal" />). Users can still add entries to it.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete</emphasis></para>

              <para>Marks the job as deleted. This will remove it from the WUI
              but not from disk.</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>

      <note>
        <para>It is not possible to directly access the configuration for
        completed jobs in the same way as you can for new, pending and running
        jobs. Instead users can look at the actual XML configuration file
        <emphasis>or</emphasis> create a new job based on the old one. The new
        job (and it need never be run) will perfectly mirror the settings of
        the old one.</para>
      </note>
    </sect2>

    <sect2 id="logs">
      <title>Logs</title>

      <para>Heritrix writes several logs as it crawls a job. Each crawl job
      has its own set of these logs.</para>

      <para>The location where logs are written can be configured (expert
      setting). Otherwise refer to the <literal>crawl-manifest.txt</literal>
      for on disk location of logs (<xref
      linkend="crawl-manifest.txt" />).</para>

      <para>Logs can be manually rotated. Pause the crawl and at the base of
      the screen a <emphasis role="bold">Rotate Logs</emphasis> link will
      appear. Clicking on <emphasis role="bold">Rotate Logs</emphasis> will
      move aside all current crawl logs appending a 14-digit GMT timestamp to
      the moved-aside logs. New log files will be opened for the crawler to
      use in subsequent crawling.</para>

      <para>The WUI offers users four ways of viewing these logs by:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Line number</emphasis></para>

          <para>View a section of a log that starts at a given line number and
          the next X lines following it. X is configurable, is 50 by
          default.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Time stamp</emphasis></para>

          <para>View a section of a log that starts at a given time stamp and
          the next X lines following it. X is configurable, is 50 by default.
          The format of the time stamp is the same as in the logs
          (YYYY-MM-DDTHH:MM:SS.SSS). It is not necessary to add more detail to
          this then is desired. For instance the entry 2004-04-25T08 will
          match the first entry made after 8 am on the 25 of April,
          2004.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Regular expression</emphasis></para>

          <para>Filter the log based on a regular expression. Only lines
          matching it (and optionally lines following it that are indented -
          usually meaning that they are related to the previous ones) are
          displayed.</para>

          <para>This can be an expensive operation on really big logs,
          requiring a lot of time for the page to load.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Tail</emphasis></para>

          <para>Allows users to just look at the last X lines of the given
          log. X can be configured, is 50 by default.</para>
        </listitem>
      </orderedlist>

      <sect3 id="crawllog">
        <title>crawl.log</title>

        <para>For each URI tried will get an entry in the
        <literal>crawl.log</literal> regardless of success or failure.</para>

        <para>Below is a two line extract from a crawl.log:</para>

        <para><programlisting>2004-07-21T23:29:40.438Z   200        310 http://127.0.0.1:9999/selftest/Charset/charsetselftest_end.html LLLL http://127.0.0.1:9999/selftest/Charset/shiftjis.jsp text/html #000 20040721232940401+10 M77KNTBZH2IU6V2SIG5EEG45EJICNQNM -
2004-07-21T23:29:40.502Z   200        225 http://127.0.0.1:9999/selftest/MaxLinkHops/5.html LLLLL http://127.0.0.1:9999/selftest/MaxLinkHops/4.html text/html #000 20040721232940481+12 M77KNTBZH2IU6V2SIG5EEG45EJICNQNM -</programlisting></para>

        <para>The <emphasis rule="bold">1st</emphasis> column is a timestamp
        in ISO8601 format, to millisecond resolution. The time is the instant
        of logging. The <emphasis rule="bold">2nd</emphasis> column is the
        fetch status code. Usually this is the HTTP status code but it can
        also be a negative number if URL processing was unexpectedly
        terminated. See <xref linkend="statuscodes" /> for a listing of
        possible values.</para>

        <para>The <emphasis rule="bold">3rd</emphasis> column is the size of
        the downloaded document in bytes. For HTTP, Size is the size of the
        content-only. It excludes the size of the HTTP response headers. For
        DNS, its the total size of the DNS response. The <emphasis
        rule="bold">4th</emphasis> column is the URI of the document
        downloaded. The <emphasis rule="bold">5th</emphasis> column holds
        breadcrumb codes showing the trail of downloads that got us to the
        current URI. See <xref linkend="discoverypath" /> for description of
        possible code values. The <emphasis rule="bold">6th</emphasis> column
        holds the URI that immediately referenced this URI ('referrer'). Both
        of the latter two fields -- the discovery path and the referrer URL --
        will be empty for such as the seed URIs.</para>

        <para>The <emphasis rule="bold">7th</emphasis> holds the document mime
        type, the <emphasis rule="bold">8th</emphasis> column has the id of
        the worker thread that downloaded this document, the <emphasis
        rule="bold">9th</emphasis> column holds a timestamp (in RFC2550/ARC
        condensed digits-only format) indicating when a network fetch was
        begun, and if appropriate, the millisecond duration of the fetch,
        separated from the begin-time by a '+' character.</para>

        <para>The <emphasis rule="bold">10th</emphasis> field is a SHA1 digest
        of the content only (headers are not digested). The <emphasis
        rule="bold">11th</emphasis> column is the 'source tag' inherited by
        this URI, if that feature is enabled. Finally, the <emphasis
        rule="bold">12th</emphasis> column holds <quote>annotations</quote>,
        if any have been set. Possible annontations include: the number of
        times the URI was tried (This field is '-' if the download was never
        retried); the literal <literal>lenTrunc</literal> if the download was
        truncated because it exceeded configured limits;
        <literal>timeTrunc</literal> if the download was truncated because the
        download time exceeded configured limits; or
        <literal>midFetchTrunc</literal> if a midfetch filter determined the
        download should be truncated.</para>
      </sect3>

      <sect3>
        <title>local-errors.log</title>

        <para>Errors that occur when processing a URI that can be handled by
        the processors (usually these are network related problems trying to
        fetch the document) are logged here.</para>

        <para>Generally these can be safely ignored, but can provide insight
        to advanced users when other logs and/or reports have unusual
        data.</para>
      </sect3>

      <sect3>
        <title>progress-statistics.log</title>

        <para>This log is written by the <literal>StatisticsTracker</literal>
        (<xref linkend="stattrack" />).</para>

        <para>At configurable intervals a line about the progress of the crawl
        is written to this file.</para>

        <para>The legends are as follows:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">timestamp</emphasis></para>

            <para>Timestamp indicating when the line was written, in ISO8601
            format.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">discovered</emphasis></para>

            <para>Number of URIs discovered to date.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">queued</emphasis></para>

            <para>Number of URIs queued at the moment.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">downloaded</emphasis></para>

            <para>Number of URIs downloaded to date</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">doc/s(avg)</emphasis></para>

            <para>Number of documents downloaded per second since the last
            snapshot. In parenthesis since the crawl began.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">KB/s(avg)</emphasis></para>

            <para>Amount in Kilobytes downloaded per second since the last
            snapshot. In parenthesis since the crawl began.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">dl-failures</emphasis></para>

            <para>Number of URIs that Heritrix has failed to download to
            date.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">busy-thread</emphasis></para>

            <para>Number of toe threads currently busy processing a
            URI.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">mem-use-KB</emphasis></para>

            <para>Amount of memory currently assigned to the Java Virtual
            Machine.</para>
          </listitem>
        </itemizedlist>
      </sect3>

      <sect3>
        <title>runtime-errors.log</title>

        <para>This log captures unexpected exceptions and errors that occur
        during the crawl. Some may be due to hardware limitation (out of
        memory, although that error may occur without being written to this
        log), but most are probably because of software bugs, either in
        Heritrix's core but more likely in one of the pluggable
        classes.</para>
      </sect3>

      <sect3>
        <title>uri-errors.log</title>

        <para>Contains errors in dealing with encountered URIs. Usually its
        caused by erroneous URIs. Generally only of interest to advanced users
        trying to explain unexpected crawl behavior.</para>
      </sect3>

      <sect3 id="recover_gz">
        <title>recover.gz</title>

        <para>The recover.gz file is a gzipped journal of Frontier events. It
        can be used to restore the Frontier after a crash to roughly the state
        it had before the crash. See <xref linkend="recover" /> to learn
        more.</para>
      </sect3>
    </sect2>

    <sect2 id="reports">
      <title>Reports</title>

      <para>Heritrix's WUI offers a couple of reports on ongoing and completed
      crawl jobs.</para>

      <para>Both are accessible via the Reports tab.<note>
          <para>Although jobs are loaded after restarts of the software, their
          statistics are not reloaded with them. That means that these reports
          are only available as long as Heritrix is not shut down. All of the
          information is however replicated in report files at the end of each
          crawl for permanent storage.</para>
        </note></para>

      <sect3 id="crawlreport">
        <title>Crawl report</title>

        <para>At the top of the crawl report some general statistics about the
        crawl are printed out. All of these replicate data from the Console so
        you should refer to <xref linkend="console" /> for more information on
        them.</para>

        <para>Next in line are statistics about the number of URIs pending,
        discovered, currently queued, downloaded etc. Question marks after
        most of the values provides pop up descriptions of those
        metrics.</para>

        <para>Following that is a breakdown of the distribution of status
        codes among URIs. It is sorted from most frequent to least. The number
        of URIs found for each status code is displayed. Only successful
        fetches are counted here.</para>

        <para>A similar breakdown for file types (mime types) follows. In
        addition to the number of URIs per file type, the amount of data for
        that file type is also displayed.</para>

        <para>Last a breakdown per host is provided. Number of URIs and amount
        of data for each is presented. The time that has elapsed since the
        last URI was finished for each host is also displayed for ongoing
        crawls. This value can provide valuable data on what hosts are still
        being actively crawled. Note that this value is only available while
        the crawl is in progress since it has no meaning afterwards. Also any
        pauses made to the crawl may distort these values, at least in the
        short term following resumption of crawling. Most noticably while
        paused all of these values will continue to grow.</para>

        <para>Especially in broad crawls, this list can grow very
        large.</para>
      </sect3>

      <sect3 id="seedsreport">
        <title>Seeds report</title>

        <para>This report lists all the seeds in the seeds file and also any
        <emphasis>discovered</emphasis> seeds if that option is enabled (that
        is treat redirects from seeds as new seeds). For each seed the status
        code for the fetch attempt is presented in verbose form (that is with
        minimum textual description of its meaning). Following that is the
        seeds disposition, a quick look at if the seed was successfully
        crawled, not attempted, or failed to crawl.</para>

        <para>Successfully crawled seeds are any that Heritrix had no internal
        errors crawling, the seed may never the less have generated a 404
        (file not found) error.</para>

        <para>Failure to crawl might be because of a bug in Heritrix or an
        invalid seed (commonly DNS lookup will have failed).</para>

        <para>If the report is examined before the crawl is finished there
        might still be seeds not yet attempted. Especially if there is trouble
        getting their prerequisites or if the seed list is exceptionally
        large.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="outside">
    <title>Outside the user interface</title>

    <para>While it is possible to do a great many things via Heritrix's WUI it
    is worth taking a look at some of what is not available in it.</para>

    <sect2>
      <title>Generated files</title>

      <para>In addition to the logs discussed above (see <xref
      linkend="logs" />) the following files are generated. Some of the
      information in them is also available via the WUI.</para>

      <sect3>
        <title>heritrix_out.log</title>

        <para>Captures what is written to the standard out and standard error
        streams of the program. Mostly this consists of low level exceptions
        (usually indicative of bugs) and also some information from third
        party modules who do their own output logging.</para>

        <para>This file is created in the same directory as the Heritrix JAR
        file. It is not associated with any one job, but contains output from
        all jobs run by the crawler.</para>
      </sect3>

      <sect3 id="crawl-manifest.txt">
        <title>crawl-manifest.txt</title>

        <para>A manifest of all files (excluding ARC and other data files)
        created while crawling a job.</para>

        <para>An example of this file might be:</para>

        <para><programlisting>  L+ /Heritrix/jobs/quickbroad-20040420191411593/disk/crawl.log
  L+ /Heritrix/jobs/quickbroad-20040420191411593/disk/runtime-errors.log
  L+ /Heritrix/jobs/quickbroad-20040420191411593/disk/local-errors.log
  L+ /Heritrix/jobs/quickbroad-20040420191411593/disk/uri-errors.log
  L+ /Heritrix/jobs/quickbroad-20040420191411593/disk/progress-statistics.log
  L- /Heritrix/jobs/quickbroad-20040420191411593/disk/recover.gz
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/seeds-report.txt
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/hosts-report.txt
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/mimetype-report.txt
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/responsecode-report.txt
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/crawl-report.txt
  R+ /Heritrix/jobs/quickbroad-20040420191411593/disk/processors-report.txt
  C+ /Heritrix/jobs/quickbroad-20040420191411593/job-quickbroad.xml
  C+ /Heritrix/jobs/quickbroad-20040420191411593/settings/org/settings.xml
  C+ /Heritrix/jobs/quickbroad-20040420191411593/seeds-quickbroad.txt</programlisting></para>

        <para>The first character of each line indicates the type of file. L
        for logs, R for reports and C for configuration files.</para>

        <para>The second character - a plus or minus sign - indicates if the
        file should be included in a standard bundle of the job (see <xref
        linkend="manifest_bundle.pl" />). In the example above the
        <literal>recover.gz</literal> is marked for exclusion because it is
        generally only of interest if the job crashes and must be restarted.
        It has negligible value once the job is completed (See <xref
        linkend="recover" />).</para>

        <para>After this initial legend the filename with full path
        follows.</para>

        <para>This file is generated in the directory indicated by the
        '<emphasis>disk</emphasis>' attribute of the configuration at the very
        end of the crawl.</para>
      </sect3>

      <sect3>
        <title>crawl-report.txt</title>

        <para>Contains some useful metrics about the completed jobs. This
        report is created by the <literal>StatisticsTracker</literal> (see
        <xref linkend="stattrack" />)</para>

        <para>Written at the very end of the crawl only. See
        <literal>crawl-manifest.txt</literal> for its location.</para>
      </sect3>

      <sect3>
        <title>hosts-report.txt</title>

        <para>Contains an overview of what hosts were crawled and how many
        documents and bytes were downloaded from each.</para>

        <para>This report is created by the
        <literal>StatisticsTracker</literal> (see <xref
        linkend="stattrack" />) and is written at the very end of the crawl
        only. See <literal>crawl-manifest.txt</literal> for its
        location.</para>
      </sect3>

      <sect3>
        <title>mimetype-report.txt</title>

        <para>Contains on overview of the number of documents downloaded per
        mime type. Also has the amount of data downloaded per mime
        type.</para>

        <para>This report is created by the
        <literal>StatisticsTracker</literal> (see <xref
        linkend="stattrack" />) and is written at the very end of the crawl
        only. See <literal>crawl-manifest.txt</literal> for its
        location.</para>
      </sect3>

      <sect3 id="processorsreport.txt">
        <title>processors-report.txt</title>

        <para>Contains the processors report (see <xref
        linkend="processorsreport" />) generated at the very end of the
        crawl.</para>
      </sect3>

      <sect3>
        <title>responsecode-report.txt</title>

        <para>Contains on overview of the number of documents downloaded per
        status code (see <xref linkend="statuscodes" />), covers successful
        codes only, does not tally failures, see <literal>crawl.log</literal>
        for that information.</para>

        <para>This report is created by the
        <literal>StatisticsTracker</literal> (see <xref
        linkend="stattrack" />) and is written at the very end of the crawl
        only. See <literal>crawl-manifest.txt</literal> for its
        location.</para>
      </sect3>

      <sect3>
        <title>seeds-report.txt</title>

        <para>An overview of the crawling of each seed. Did it succeed or not,
        what status code was returned.</para>

        <para>This report is created by the
        <literal>StatisticsTracker</literal> (see <xref
        linkend="stattrack" />) and is written at the very end of the crawl
        only. See <literal>crawl-manifest.txt</literal> for its
        location.</para>
      </sect3>

      <sect3 id="arcfiles">
        <title>ARC files</title>

        <para>Assuming that you are using the ARC writer that comes with
        Heritrix a number of ARC files will be generated containing the
        crawled pages.</para>

        <para>It is possible to specify the location of these files on the
        ARCWriter processor in settings page. Unless this is set as an
        absolute path this is a path relative to the <emphasis>job</emphasis>
        directory.</para>

        <para>ARC files are named as follows:</para>

        <para><programlisting>  [prefix]-[12-digit-timestamp]-[series#-padded-to-5-digits]-[crawler-hostname].arc.gz</programlisting></para>

        <para>The <literal>prefix</literal> is set by the user when he
        configures the ARCWriter processor. By default it is IAH.</para>

        <para>If you see an ARC file with an extra <literal>.open</literal>
        suffix, this means the ARC is currently in use being written to by
        Heritrix (It usually has more than one ARC open at a time).</para>

        <para id="invalid">Files with a <literal>.invalid</literal> are files
        Heritrix had trouble writing to (Disk full, bad disk, etc.). On
        IOException, Heritrix closes the problematic ARC and gives it the
        <literal>.invalid</literal> suffix. These files need to be checked for
        coherence.</para>

        <para>For more on ARC files refer to the <ulink
        url="http://crawler.archive.org/apidocs/org/archive/io/arc/ARCWriter.html">ARCwriter
        Javadoc</ulink> and to the <ulink
        url="http://crawler.archive.org/articles/developer_manual/arcs.html">ARC
        Writer developer documentation</ulink>.</para>
      </sect3>
    </sect2>

    <sect2>
      <title>Helpful scripts</title>

      <para>Heritrix comes bundled with a few helpful scripts for
      Linux.</para>

      <sect3 id="manifest_bundle.pl">
        <title>manifest_bundle.pl</title>

        <para>This script will bundle up all resources referenced by a crawl
        manifest file (<xref linkend="crawl-manifest.txt" />). Output bundle
        is an uncompressed or compressed tar ball. Directory structure created
        in the tar ball is as follow:</para>

        <itemizedlist>
          <listitem>
            <para>Top level directory (crawl name)</para>
          </listitem>

          <listitem>
            <para>Three default subdirectories (configuration, logs and
            reports directories)</para>
          </listitem>

          <listitem>
            <para>Any other arbitrary subdirectories</para>
          </listitem>
        </itemizedlist>

        <para>Usage:<programlisting>  manifest_bundle.pl crawl_name manifest_file [-f output_tar_file] [-z] [ -flag directory]
      -f output tar file. If omitted output to stdout.
      -z compress tar file with gzip.
      -flag is any upper case letter. Default values C, L, and are R are set to
       configuration, logs and reports</programlisting></para>

        <para>Example:<programlisting>  manifest_bundle.pl testcrawl crawl-manifest.txt -f    \
        /0/testcrawl/manifest-bundle.tar.gz -z -F filters</programlisting></para>

        <para>Produced tar ball for this example:<programlisting>  /0/testcrawl/manifest-bundle.tar.gz</programlisting>Bundled
        directory structure for this example:<programlisting>  |-testcrawl
      |- configurations
      |- logs
      |- reports
      |- filters</programlisting></para>
      </sect3>

      <sect3 id="hoppath">
        <title>hoppath.pl</title>

        <para>This perl script, found in $HERITRIX_HOME/bin recreates the hop
        path to the specified url. The hop path is a path of links (URLs) that
        we followed to get to the specified url.</para>

        <para>Usage:<programlisting>Usage: hoppath.pl crawl.log URI_PREFIX
  crawl.log    Full-path to Heritrix crawl.log instance.
  URI_PREFIX   URI we're querying about. Must begin 'http(s)://' or 'dns:'.
               Wrap this parameter in quotes to avoid shell interpretation
               of any '&amp;' present in URI_PREFIX.</programlisting></para>

        <para>Example:<programlisting>% hoppath.pl crawl.log 'http://www.house.gov/'</programlisting></para>

        <para>Result:<programlisting>  2004-02-25-02-36-06 - http://www.house.gov/house/MemberWWW_by_State.html
  2004-02-25-02-36-06   L http://wwws.house.gov/search97cgi/s97_cgi
  2004-02-25-03-30-38    L http://www.house.gov/</programlisting></para>

        <para>The L in the above example refers to the type of link followed
        (see <xref linkend="discoverypath" />).</para>
      </sect3>

      <sect3 id="recoverylogmapper">
        <title>RecoverLogMapper</title>

        <para><literal>org.archive.crawler.util.RecoveryLogMapper</literal> is
        similar to <xref linkend="hoppath" />. It was contributed by Mike
        Schwartz. RecoverLogMapper parses a Heritrix recovery log file, (See
        <xref linkend="recover" />), and builds maps that allow a caller to
        look up any seed URL and get back an Iterator of all URLs successfully
        crawled from given seed. It also allows lookup on any crawled URL to
        find the seed URL from which the crawler reached that URL (through 1
        or more discovered URL hops, which are collapsed in this
        lookup).</para>
      </sect3>

      <sect3 id="jmxclient">
        <title>cmdline-jmxclient</title>

        <para>This jar file is checked in as a script. It enables command-line
        control of Heritrix if Heritrix has been started up inside of a SUN
        1.5.0 JDK. See the <ulink
        url="http://crawler.archive.org/cmdline-jmxclient/">cmdline-jmxclient
        project</ulink> to learn more about this script's capabilities and how
        to use it. See also <xref linkend="mon_com" />.</para>
      </sect3>
    </sect2>

    <sect2 id="recover">
      <title>Recovery of Frontier State and recover.gz</title>

      <para>During normal running, the Heritrix Frontier by default keeps a
      journal. The journal is kept in the jobs logs directory. Its named
      <literal>recover.gz</literal>. If a crawl crashes, the recover.gz
      journal can be used to recreate approximately the status of the crawler
      at the time of the crash. Recovery can take a long time in some cases,
      but is usually much quicker then repeating a crawl.</para>

      <para>To run the recovery process, relaunch the crashed crawler. Create
      a new crawl order job based on the crawl that crashed. If you choose the
      "recover-log" link from the list of completed jobs in the 'Based on
      recovery' page, the new job will automatically be set up to use the
      original job's recovery journal to bootstrap its Frontier state (of
      completed and queued URIs). Further, if the recovered job attempts to
      reuse any already-full 'logs' or 'state' directories, new paths for
      these directories will be chosen with as many '-R' suffixes as are
      necessary to specify a new empty directory.</para>

      <para>(If you simply base your new job on the old job without using the
      'Recover' link, you must manually enter the full path of the original
      crawl's recovery journal into the <literal>recover-path</literal>
      setting, near the end of all settings. You must also adjust the 'logs'
      and 'state' directory settings, if they were specified using absolute
      paths that would cause the new crawl to reuse the directories of the
      original job.)</para>

      <para>After making any further adjustments to the crawl settings, submit
      the new job. The submission will hang a long time as the recover.gz file
      is read in its entirety by the frontier. (This can take hours for a
      crawl that has run for a long time, and during this time the crawler
      control panel will appear idle, with no job pending or in progress, but
      the machine will be busy.) Eventually the submit and crawl job launche
      should complete. The crawl should pick up from close to where the crash
      occurred. There is no marking in the logs that this crawl was started by
      reading a recover log (Be sure to mark this was done in the crawl
      journal).</para>

      <para>The recovery log is gzipped because it gets very large otherwise
      and because of the reptition of terms, it compresses very well. On
      abnormal termination of the crawl job, if you look at the recover.gz
      file with gzip, gzip will report <literal>unexpected end of
      file</literal> if you try to ungzip it. Gzip is complaining that the
      file write was abnormally terminated. But the recover.gz file will be of
      use restoring the frontier at least to where the gzip file went bad
      (Gzip zips in 32k blocks; the worst loss would be the last 32k of
      gzipped data).</para>

      <para>Java's Gzip support (up through at least Java 1.5/5.0) can
      compress arbitrarily large input streams, but has problems when
      decompressing any stream to output larger than 2GB. When attempting to
      recover a crawl that has a recovery log that, when uncompressed, would
      be over 2GB, this will trigger a FatalConfigurationException alert, with
      detail message "Recover.log problem: java.io.IOException: Corrupt GZIP
      trailer". Heritrix will accept either compressed or uncompressed
      recovery log files, so a workaround is to first uncompress the recovery
      log using another non-Java tool (such as the 'gunzip' available in Linux
      and Cygwin), then refer to this uncompressed recovery log when
      recovering. (Reportedly, Java 6.0 "Mustang" will fix this Java bug with
      un-gzipping large files.)</para>

      <para>See also below, the related recovery facility, <xref
      linkend="checkpoint" />, for an alternate recovery mechanism.</para>
    </sect2>

    <sect2 id="checkpoint">
      <title>Checkpointing</title>

      <para>Checkpointing [<xref linkend="checkpointing" />], the crawler
      writes a representation of current state to a directory under
      <literal>checkpoints-path</literal>, named for the checkpoint.
      Checkpointed state includes serialization of main crawler objects,
      copies of current set of bdbje log files, etc. The idea is that the
      checkpoint directory contains all that is required recovering a crawler.
      Checkpointing also rotates off the crawler logs including the
      <literal>recover.gz</literal> log, if enabled. Log files are NOT copied
      to the checkpoint directory. They are left under the logs directory but
      are distingushed by a suffix. The suffix is the checkpoint name (e.g.
      <literal>crawl.log.000031</literal> where <literal>000031</literal> is
      the checkpoint name).</para>

      <note>
        <para>Currently, only the BdbFrontier using the bdbje-based
        already-seen or the bloom filter already-seen is
        checkpointable.</para>
      </note>

      <para>To run a checkpoint, click the checkpoint button in the UI or
      invoke <literal>checkpoint</literal> from JMX. This launches a thread to
      run through the following steps: If crawling, pause the crawl; run
      actual checkpoint; if was crawling when checkpoint was invoked, resume
      the crawl. Dependent on the size of the crawl, checkpointing can take
      some time; often the step that takes longest is pausing the crawl,
      waiting on threads to get into a paused, checkpointable state. While
      checkpointing, the status will show as <literal>CHECKPOINTING</literal>.
      When the checkpoint has completed -- the crawler will resume crawling
      (Of if in PAUSED state when checkpointing was invoked, will return to
      the PAUSED state).</para>

      <para>Recovery from a checkpoint has much in common with the recovery of
      a crawl using the recover.log (See <xref linked="recover"></xref>. To
      recover, create a job. Then before launching, set the
      <literal>crawl-order/recover-path</literal> to point at the checkpoint
      directory you want to recover from. Alternatively, browse to the
      <literal>Jobs-&gt;Based on a recovery</literal> screen and select the
      checkpoint you want to recover from. After clicking, a new job will be
      created that takes the old jobs' (end-of-crawl) settings and autofills
      the recover-path with the right directory-path (The renaming of logs and
      <literal>crawl-order/state-path</literal> "state" dirs so they do not
      clash with the old as is described above in <xref linkend="recover" />
      is also done). The first thing recover does is copy into place the
      saved-off bdbje log files. Again, recovery can take time -- an hour or
      more if a crawl of millions.</para>

      <para>Checkpointing is currently experimental. The recover-log technique
      is tried-and-true. Once checkpointing is proven reliable, faster, and
      more comprehensive, it will become the preferred method recovering a
      crawler).</para>

      <sect3>
        <title>Expert mode: Fast checkpointing</title>

        <para>The bulk of the time checkpointing is taken up copying off the
        bdbje logs. For example, checkpointing a crawl that had downloaded
        18million items -- it had discovered &gt; 130million (bloom filter) --
        the checkpointing took about 100minutes to complete of which 90 plus
        minutes were spent copying the ~12k bdbje log files (One disk only
        involved). Set log level on
        <literal>org.archive.util.FileUtils</literal> to FINE to watch the
        java bdbje log file-copy.</para>

        <para>Since copying off bdbje log files can take hours, we've added an
        <emphasis>expert mode</emphasis> checkpoint that bypasses bdbje log
        copying. The upside is your checkpoint completes promptly -- in
        minutes, even if the crawl is large -- but downside is recovery takes
        more work: to recover from a checkpoint, the bdbje log files need to
        be manually assembled in the checkpoint <literal>bdb-logs</literal>
        subdirectory. You'll know which bdbje log files make up the checkpoint
        because Heritrix writes the checkpoint list of bdbje logs into the
        checkpoint directory to a file named
        <literal>bdbj-logs-manifest.txt</literal>. To prevent bdbje removing
        log files that might be needed assembling a checkpoint made at
        sometime in the past, when running expert mode checkpointing, we
        configure bdbje not to delete logs when its finished with them;
        instead, bdbje gives logs its no longer using a
        <literal>.del</literal> suffix. Assembling a checkpoint will often
        require renaming files with the <literal>.del</literal> suffix so they
        have the <literal>.jdb</literal> suffix in accordance with the
        <literal>bdbj-logs-manifest.txt</literal> list (See below for more on
        this).</para>

        <note>
          <para>With this expert mode enabled, the crawler
          <literal>crawl-order/state-path</literal> "state" directory will
          grow without bound; a process external to the crawler can be set to
          prune the state directory of <literal>.del</literal> files
          referenced by checkpoints since superceded).</para>
        </note>

        <para>To enable the no-files copy checkpoint, set the new expert mode
        setting <literal>checkpoint-copy-bdbje-logs</literal> to
        <literal>false</literal>.</para>

        <para>To recover using a checkpoint that has all but the bdbje log
        files present, you will need to copy all logs listed in
        <literal>bdbj-logs-manifest.txt</literal> to the
        <literal>bdbje-logs</literal> checkpoint subdirectory. In some cases
        this will necessitate renaming logs with the <literal>.del</literal>
        to instead have the <literal>.jdb</literal> ending as suggested above.
        One thing to watch for is copying too many logs into the bdbje logs
        subdirectory. The list of logs must match exactly whats in the
        manifest file. Otherwise, the recovery will fail (For example, see
        <ulink
        url="http://sourceforge.net/tracker/index.php?func=detail&amp;aid=1325961&amp;group_id=73833&amp;atid=539099">[1325961]
        resurrectOneQueueState has keys for items not in
        allqueues</ulink>).</para>

        <para>On checkpoint recovery, Heritrix copies bdbje log files from the
        referenced checkpoint <literal>bdb-logs</literal> subdirectory to the
        new crawl's <literal>crawl-order/state-path</literal> "state"
        directory. As noted above, this can take some time. Of note, if a
        bdbje log file already exists in the new crawls'
        <literal>crawl-order/state-path</literal> "state" directory,
        checkpoint recover will not overwrite the existing bdbje log file.
        Exploit this property and save on recovery time by using native unix
        <literal>cp</literal> to manually copy over bdbje log files from the
        checkpoint directory to the new crawls'
        <literal>crawl-order/state-path</literal> "state" directory before
        launching a recovery (Or, at the extreme, though it will trash your
        checkpoint, set the checkpoint's <literal>bdb-logs</literal>
        subdirectory as the new crawls
        <literal>crawl-order/state-path</literal> "state" directory).</para>
      </sect3>

      <sect3 id="automated_chkpt">
        <title>Automated Checkpointing</title>

        <para>To have Heritrix run a checkpoint on a period, uncomment (or
        add) to <literal>heritrix.properties</literal> a line like:
        <programlisting>org.archive.crawler.framework.Checkpointer.period = 2</programlisting>
        This will install a Timer Thread that will run on an interval (Units
        are in hours). See <literal>heritrix_out.log</literal> to see log of
        installation of the timer thread that will run the checkpoint on a
        period and to see log of everytime it runs (Assuming
        <literal>org.archive.crawler.framework.Checkpointer.level</literal> is
        set to INFO).</para>
      </sect3>
    </sect2>

    <sect2 id="mon_com">
      <title>Remote Monitoring and Control</title>

      <para>As of release 1.4.0, Heritrix will start up the JVM's JMX Agent if
      deployed in a SUN 1.5.0 JVM. It password protects the JMX Agent using
      whatever was specified as the Heritrix admin password so to login,
      you'll use 'monitorRole' or 'controlRole' for login and the Heritrix
      admin password as password. By default, the JMX Agent is started up on
      port 8849 (To change any of the JMX settings, set the JMX_OPTS
      environment variable).</para>

      <para>On startup, Heritrix looks if any JMX Agent running in current
      context and registers itself with the first JMX Agent found publishing
      attributes and operations that can be run remotely. If running in a SUN
      1.5.0 JVM where the JVM JMX Agent has been started, Heritrix will attach
      to the JVM JMX Agent (If running inside JBOSS, Heritrix will register
      with the JBOSS JMX Agent).</para>

      <para>To see what Attributes and Operations are available via JMX, use
      the SUN 1.5.0 JDK jconsole application -- its in $JAVA_HOME/bin -- or
      use <xref linkend="jmxclient" />.</para>

      <para>To learn more about the SUN 1.5.0 JDK JMX managements and
      jconsole, see <ulink
      url="http://java.sun.com/j2se/1.5.0/docs/guide/management/agent.html">Monitoring
      and Management Using JMX</ulink>. This O'Reilly article is also a good
      place for getting started : <ulink
      url="http://www.onjava.com/pub/a/onjava/2004/09/29/tigerjmx.html">Monitoring
      Local and Remote Applications Using JMX 1.2 and JConsole</ulink>.</para>
    </sect2>

    <sect2 id="ftp_support">
      <title>Experimental FTP Support</title>

      <para>As of release 1.10.0, Heritrix has experimental support for
      crawling FTP servers. To enable FTP support for your crawls, there is a
      configuration file change you will have to manually make.</para>

      <para>Specifically, you will have to edit the
      <filename>$HERITRIX_HOME/conf/heritrix.properties</filename> file.
      Remove <literal>ftp</literal> from the
      <literal>org.archive.net.UURIFactory.ignored-schemes</literal> property
      list. Also, you must add <literal>ftp</literal> to the
      <literal>org.archive.net.UURIFactory.schemes</literal> property
      list.</para>

      <para>After that change, you should be able to add the FetchFTP
      processor to your crawl using the Web UI. Just create a new job, click
      "Modules", and add FetchFTP under "Fetchers."</para>

      <para>Note that FetchFTP is a little unusual in that it works both as a
      fetcher and as an extractor. If an FTP URI refers to a directory, and if
      FetchFTP's <literal>extract-from-dirs</literal> property is set to true,
      then FetchFTP will extract one link for every line of the directory
      listing. Similarly, if the <literal>extract-parent</literal> property is
      true, then FetchFTP will extract the parent directory from every FTP URI
      it encounters.</para>

      <para>Also, remember that FetchFTP is experimental. As of 1.10, FetchFTP
      has the following known limitations:</para>

      <orderedlist>
        <listitem>
          FetchFTP can only store directories if the FTP server supports the 

          <literal>NLIST</literal>

           command. Some older systems may not support 

          <literal>NLIST</literal>

          .
        </listitem>

        <listitem>
          Similarly, FetchFTP uses passive mode transfer, to work behind firewalls. Not all FTP servers support passive mode, however.
        </listitem>

        <listitem>
          Heritrix currently has no means of determining the mime-type of a document unless an HTTP server explicitly mentions one. Since FTP has no concept of metadata, all documents retrieved using FetchFTP have a mime-type of 

          <literal>no-type</literal>

          . 
        </listitem>

        <listitem>
          In the absence of a mime-type, many of the postprocessors will not work. For instance, HTMLExtractor will not extract links from an HTML file fetched with FetchFTP. 
        </listitem>
      </orderedlist>

      <para>Still, FetchFTP can be used to archive an FTP directory of
      tarballs, for instance. If you discover any additional problems using
      FetchFTP, please inform the
      <email>archive-crawler@yahoogroups.com</email> mailing list.</para>
    </sect2>

    <sect2>
      <title>Duplication Reduction Processors</title>

      <para>Starting in release 1.12.0, a number of Processors can cooperate
      to carry forward URI content history information between crawls,
      reducing the amount of duplicate material downloaded or stored in later
      crawls. For more information, see the project wiki's <ulink
      url="http://webteam.archive.org/confluence/display/Heritrix/Feature+Notes+-+1.12.0">notes
      on using the new duplication-reduction functionality</ulink>. </para>
    </sect2>
  </sect1>

  <appendix id="usecases">
    <title>Common Heritrix Use Cases</title>

    <appendixinfo>
      <author>
        <firstname>Frank</firstname>

        <surname>McCown</surname>

        <affiliation>
          <orgname>Old Dominion University</orgname>
        </affiliation>
      </author>
    </appendixinfo>

    <para>There are many different ways you may perform a web crawl. Here we
    have listed several use cases which will allow you to become familiar with
    some of Heritrix's more frequently used crawling parameters.</para>

    <sect2>
      <title>Avoiding Too Much Dynamic Content</title>

      <para>Suppose you want to crawl only pages from a particular host
      (<literal>http://www.foo.org/</literal>), and you want to avoid crawling
      too many pages of the dynamically generated calendar. Let's say the
      calendar is accessed by passing a year, month and day to the
      <literal>calendar</literal> directory, as in
      <literal>http://www.foo.org/calendar?year=2006&amp;month=3&amp;day=12</literal>.</para>

      <para>When you first create the job for this crawl, you will specify a
      single seed URI: <literal>http://www.foo.org/</literal>. By default,
      your new crawl job will use the DecidingScope, which will contain a
      default set of DecideRules. One of the default rules is the
      SurtPrefixedDecideRule, which tells Heritrix to accept any URIs that
      match our seed URI's SURT prefix,
      <literal>http://(org,foo,www,)/</literal>. Subsequently, if the URI
      <literal>http://foo.org/</literal> is encountered, it will be rejected
      since its SURT prefix <literal>http://(org,foo,)</literal> does not
      match the seed's SURT prefix. To allow both <literal>foo.org</literal>
      and <literal>www.foo.org</literal>, you could use the two seeds
      <literal>http://foo.org/</literal> and
      <literal>http://www.foo.org/</literal>. To allow every subdomain of
      <literal>foo.org</literal>, you could use the seed
      <literal>http://foo.org</literal> (note the absence of a trailing
      slash).</para>

      <para>You will need to delete the TransclusionDecideRule since this rule
      has the potential to lead Heritrix onto another host. For example, if a
      URI returned a 301 (moved permanently) or 302 (found) response code and
      a URI with a different host name, Heritrix would accept this URI using
      the TransclusionDecideRule. Removing this rule will keep Heritrix from
      straying off of our <literal>www.foo.org</literal> host.</para>

      <para>A few of the rules like PathologicalPathDecideRule and
      TooManyPathSegmentsDecideRule will allow Heritrix to avoid some types of
      crawler traps. The TooManyHopsDecideRule will keep Heritrix from
      following too many links away from the seed so the calendar doesn't trap
      Heritrix in an infinite loop. By default, the hop path is set to 15, but
      you can change that on the Settings screen.</para>

      <para>Alternatively, you may add the MatchesFilePatternDecideRule. Set
      <literal>use-preset-pattern</literal> to <literal>CUSTOM</literal> and
      set <literal>regexp</literal> to something like:</para>

      <computeroutput>.*foo\.org(?!/calendar).*|.*foo\.org/calendar\?year=200[56].*</computeroutput>

      <para>Finally, you'll need to set the <literal>user-agent</literal> and
      <literal>from</literal> fields on the Settings screen, and then you may
      submit the job and monitor the crawl.</para>
    </sect2>

    <sect2>
      <title>Only Store Successful HTML Pages</title>

      <para>Suppose you wanted to only grab the first 50 pages encountered
      from a set of seeds and archive only those pages that return a 200
      response code and have the <literal>text/html</literal> MIME type.
      Additionally, you only want to look for links in HTML resources.</para>

      <para>When you create your job, use the DecidingScope with the default
      set of DecideRules.</para>

      <para>In order to examine HTML documents only for links, you will need
      to remove the following extractors that tell Heritrix to look for links
      in style sheets, JavaScript, and Flash files:</para>

      <orderedlist>
        <listitem>ExtractorCSS</listitem>

        <listitem>ExtractorJS</listitem>

        <listitem>ExtractorSWF</listitem>
      </orderedlist>

      <para>You should leave in the ExtractorHTTP since it is useful in
      locating resources that can only be found using a redirect (301 or
      302).</para>

      <para>You can limit the number of files to download by setting
      max-document-download on the Settings screen. Setting this value to 50
      will probably not have the results you intend. Since each DNS response
      and robots.txt file is counted in this number, you'll likely want to use
      the value of 50 * number of seeds * 2.</para>

      <para>Next, you will need to add filters to the ARCWriterProcessor so
      that it only records documents with a 200 status code and a mime-type of
      text/html. The first filter to add is the ContentTypeRegExpFilter; set
      its <literal>regexp</literal> setting to <literal>text/html.*</literal>.
      Next, add a DecidingFilter to the ARCWriterProcessor, then add
      FetchStatusDecideRule to the DecidingFilter.</para>

      <para>You'll probably want to apply the above filters to the
      <literal>mid-fetch-filters</literal> setting of FetchHTTP as well. That
      will prevent FetchHTTP from downloading the content of any non-html or
      non-successful documents.</para>

      <para>Once you have entered the desired settings, start the job and
      monitor the crawl.</para>
    </sect2>

    <sect2>
      <title>Mirroring .html Files Only</title>

      <para>Suppose you only want to crawl URLs that match
      <literal>http://foo.org/bar/*.html</literal>, and you'd like to save the
      crawled files in a file/directory format instead of saving them in ARC
      files. Suppose you also know that you are crawling a web server that is
      case sensitive (<literal>http://foo.org/bar/abc.html</literal> and
      <literal>http://foo.org/bar/ABC.HTML</literal> are pointing to two
      different resources).</para>

      <para>You would first need to create a job with the single seed
      http://foo.org/bar/. You'll need to add the MirrorWriterProcessor on the
      Modules screen and delete the ARCWriterProcessor. This will store your
      files in a directory structure that matches the crawled URIs, and the
      files will be stored in the crawl job's <filename>mirror</filename>
      directory.</para>

      <para>Your job should use the DecidingScope with the following set of
      DecideRules:</para>

      <orderedlist>
        <listitem>RejectDecideRule</listitem>

        <listitem>SurtPrefixedDecideRule</listitem>

        <listitem>TooManyHopsDecideRule</listitem>

        <listitem>PathologicalPathDecideRule</listitem>

        <listitem>TooManyPathSegmentsDecideRule</listitem>

        <listitem>NotMatchesFilePatternDecideRule</listitem>

        <listitem>PrerequisiteAcceptDecideRule</listitem>
      </orderedlist>

      <para>We are using the NotMatchesFilePatternDecideRule so we can
      eliminate crawling any URIs that don't end with
      <literal>.html</literal>. It's important that this DecideRule be placed
      immediately before PrerequisiteAcceptDecideRule; otherwise the DNS and
      robots.txt prerequisites will be rejected since they won't match the
      regexp.</para>

      <para>On the Setting screen, you'll want to set the following for the
      NotMatchesFilePatternDecideRule:</para>

      <orderedlist>
        <listitem>decision: REJECT</listitem>

        <listitem>use-preset-pattern: CUSTOM</listitem>

        <listitem>regexp: .*(/|\.html)$</listitem>
      </orderedlist>

      <para>Note that the regexp will accept URIs that end with / as well as
      .html. If we don't accept the /, the seed URI will be rejected. This
      also allows us to accept URIs like http://foo.org/bar/dir/ which are
      likely pointing to index.html. A stricter regexp would be .*\.html$, but
      you'll need to change your seed URI if you use it. One thing to be aware
      of: if Heritrix encounters the URI http://foo.org/bar/dir where dir is a
      directory, the URI will be rejected since it is missing the terminating
      slash.</para>

      <para>Finally you'll need to allow Heritrix to differentiate between
      abc.html and ABC.HTML. Do this by removing the LowercaseRule under
      uri-canonicalization-rules on the Submodules screen.</para>

      <para>Once you have entered the desired settings, start the job and
      monitor the crawl.</para>
    </sect2>
  </appendix>

  <glossary id="glossary">
    <glossdiv>
      <title>Some definitions</title>

      <glossentry id="bytes">
        <glossterm>Bytes, KB and statistics</glossterm>

        <glossdef>
          <para>Heritrix adheres to the following conventions for displaying
          byte and bit amounts:</para>

          <para><programlisting>  Legend Type
       B Bytes
      KB Kilobytes - 1 KB = 1024 B
      MB Megabytes - 1 MB = 1024 KB
      GB Gigabytes - 1 GB = 1024 MB
  
       b bits
      Kb Kilobits - 1 Kb = 1000 b
      Mb Megabits - 1 Mb = 1000 Kb
      Gb Gigabits - 1 Gb = 1000 Mb</programlisting></para>

          <para>This also applies to all logs.</para>
        </glossdef>
      </glossentry>

      <glossentry id="checkpointing">
        <glossterm>Checkpointing</glossterm>

        <glossdef>
          <para>Heritrix checkpointing has been heavily influenced by what
          Mercator provided. In <ulink
          url="http://citeseer.nj.nec.com/najork01highperformance.html">one of
          the papers on Mercator</ulink> it is described this way:
          <quote>Checkpointing is an important part of any long-running
          process such as a web crawl. By checkpointing we mean writing a
          representation of the crawler's state to stable storage that, in the
          event of a failure, is sufficient to allow the crawler to recover
          its state by reading the checkpoint and to resume crawling from the
          exact state it was in at the time of the checkpoint. By this
          definition, in the event of a failure, any work performed after the
          most recent checkpoint is lost, but none of the work up to the most
          recent checkpoint. In Mercator, the frequency with which the
          background thread performs a checkpoint is user-configurable; we
          typically checkpoint anywhere from 1 to 4 times per
          day.</quote></para>

          <para>See <xref linkend="checkpoint" /> for discussion of the
          Heritrix implementation.</para>
        </glossdef>
      </glossentry>

      <glossentry id="crawluri">
        <glossterm>CrawlURI</glossterm>

        <glossdef>
          <para>A URI and its associated data such as parent URI, number of
          links from seed etc.</para>
        </glossdef>
      </glossentry>

      <glossentry id="dates">
        <glossterm>Dates and times</glossterm>

        <glossdef>
          <para>All times in Heritrix are GMT assuming the clock and timezone
          on the local system are correct.</para>

          <para>This means that all dates/times in logs are GMT, all dates and
          times shown in the WUI are GMT and any times or dates entered by the
          user need to be in GMT.</para>
        </glossdef>
      </glossentry>

      <glossentry id="discovereduris">
        <glossterm>Discovered URIs</glossterm>

        <glossdef>
          <para>That is any URI that has been confirmed be within 'scope'.
          This includes those that have been processed, are being processed
          and have finished processing. Does not include URIs that have been
          'forgotten' (deemed out of scope when trying to fetch, most likely
          due to operator changing scope definition).</para>

          <para><note>
              <para>This only counts discovered URIs. Since the same URI can
              (at least in most frontiers) be fetched multiple times, this
              number may be somewhat lower then the combined queued, in
              process and finished items combined due to duplicate URIs being
              queued and processed. This variance is likely to be especially
              high in Frontiers implementing 'revisit' strategies.</para>
            </note></para>
        </glossdef>
      </glossentry>

      <glossentry id="discoverypath">
        <glossterm>Discovery path</glossterm>

        <glossdef>
          <para>Each URI has a discovery path. The path contains one character
          for each link or embed followed from the seed.</para>

          <para>The character legend is as follows.</para>

          <programlisting>  R - Redirect
  E - Embed
  X - Speculative embed (aggressive/Javascript link extraction)
  L - Link
  P - Prerequisite (as for DNS or robots.txt before another URI)</programlisting>

          <para>The discovery path of seeds is an empty string.</para>
        </glossdef>
      </glossentry>

      <glossentry id="glossary_frontier">
        <glossterm>Frontier</glossterm>

        <glossdef>
          <para>A Frontier is a pluggable module in Heritrix that maintains
          the internal state of the crawl. See <xref
          linkend="frontier" />.</para>
        </glossdef>
      </glossentry>

      <glossentry id="holdingvcrawling">
        <glossterm>"Holding Jobs" vs. "Crawling Jobs"</glossterm>

        <glossdef>
          <para>The mode <emphasis>Crawling Jobs</emphasis> generally means
          that the crawler will start executing a job as soon as one is made
          available in the pending jobs queue (as long as there is not a job
          already running).</para>

          <para>If the crawler is in the <emphasis>Holding Jobs</emphasis>
          mode, jobs added to the pending jobs queue will be held; they will
          not be started, even if there are no jobs currently being
          run.</para>
        </glossdef>
      </glossentry>

      <glossentry>
        <glossterm id="host">Host</glossterm>

        <glossdef>
          <para>A host can serve multiple domains or a single domain can be
          served by multiple hosts. For our purposes so far, host == hostname
          in URI. DNS is not considered; it is volatile and may be
          unavailable. So when Heritrix gets the URIs...<programlisting>  http://www.example.com
  http://search.example.com
  http://201.199.7.15
</programlisting>...even if they all point to the 201.199.7.15 IP, they are 3
          different logical hosts (at the level of the URI/HTTP
          protocol).</para>

          <para>Conformant HTTP proxies behave similarly, we think, even if
          they know www.example.com == 201.199.7.15, they will not consider
          them interchangeable.</para>

          <para>This is not ideal for politeness where we'd want politeness
          rules to apply to the physical host rather than the logical.</para>
        </glossdef>
      </glossentry>

      <glossentry id="link-hop-count">
        <glossterm>Link hop count</glossterm>

        <glossdef>
          <para>Number of link follow from the seed to the current URI. Seeds
          have a link hop count of 0.</para>

          <para>This number is equal to counting the 'L's in a URIs discovery
          path.</para>
        </glossdef>
      </glossentry>

      <glossentry>
        <glossterm>Pending URIs</glossterm>

        <glossdef>
          <para>Number of URIs that are awaiting detailed processing.</para>

          <para>Number of discovered URIs that have not been inspected for
          scope or duplicates. Depending on the implementation of the Frontier
          this might always be zero. It may also be an adjusted number that
          tries to account for duplicates by estimation.</para>
        </glossdef>
      </glossentry>

      <glossentry id="politeness">
        <glossterm>Politeness</glossterm>

        <glossdef>
          <para>Politeness refers to attempts by the crawler software to limit
          load on a site. Without politeness restrictions the crawler might
          otherwise overwhelm smaller sites and even cause moderately sized
          sites to slow down significantly.</para>

          <para>Unless you have express permission to crawl a site
          aggressively you should apply strict politeness rules to any
          crawl.</para>
        </glossdef>
      </glossentry>

      <glossentry id="queueduris">
        <glossterm>Queued URIs</glossterm>

        <glossdef>
          <para>Number of URIs queued up and waiting for processing.</para>

          <para>This includes any URIs that failed but will be retried.
          Basically this is any discovered URI that has not either been
          processed or is being processed.</para>
        </glossdef>
      </glossentry>

      <glossentry id="regexpr">
        <glossterm>Regular expressions</glossterm>

        <glossdef>
          <para>All regular expressions used by Heritrix are Java regular
          expressions.</para>

          <para>Java regular expressions differ from those used in Perl, for
          example, in several ways. For detailed info on Java regular
          expressions see the Java API for
          <literal>java.util.regex.Pattern</literal> on Sun's home page
          (<ulink url="http://java.sun.com">java.sun.com</ulink>).</para>

          <para>For API of Java SE v1.4.2 see <ulink
          url="http://java.sun.com/j2se/1.4.2/docs/api/index.html">http://java.sun.com/j2se/1.4.2/docs/api/index.html</ulink>.
          It is recommended you lookup the API for the version of Java that is
          being used to run Heritrix.</para>
        </glossdef>
      </glossentry>

      <glossentry id="server">
        <glossterm>Server</glossterm>

        <glossdef>
          <para>A server is a service on a <xref linkend="host" />. There
          might be more than one service on a host differentiated by port
          number.</para>
        </glossdef>
      </glossentry>

      <glossentry id="statuscodes">
        <glossterm>Status codes</glossterm>

        <glossdef>
          <para>Each crawled URI gets a status code. This code (or number) is
          an indication of what happened when Heritrix tried to fetch the
          URI.</para>

          <para>Codes ranging from 200 to 599 are standard HTTP response codes
          and information about their meanings is available at the <ulink
          url="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">World
          Wide Web consortium's web page</ulink>.</para>

          <para>Other status codes used by Heritrix (From <ulink
          url="http://crawler.archive.org/xref/org/archive/crawler/datamodel/FetchStatusCodes.html#38">org.archive.crawler.datamodel.FetchStatusCodes</ulink>):<programlisting>   Code Meaning
      1 Successful DNS lookup
      0 Fetch never tried (perhaps protocol unsupported or illegal URI)
     -1 DNS lookup failed
     -2 HTTP connect failed
     -3 HTTP connect broken
     -4 HTTP timeout (before any meaningful response received)
     -5 Unexpected runtime exception; see runtime-errors.log
     -6 Prerequisite domain-lookup failed, precluding fetch attempt
     -7 URI recognized as unsupported or illegal
     -8 Multiple retries all failed, retry limit reached
    -50 Temporary status assigned URIs awaiting preconditions; appearance in
        logs may be a bug
    -60 Failure status assigned URIs which could not be queued by the 
        Frontier (and may in fact be unfetchable)
    -61 Prerequisite robots.txt-fetch failed, precluding a fetch attempt
    -62 Some other prerequisite failed, precluding a fetch attempt
    -63 A prerequisite (of any type) could not be scheduled, precluding a 
        fetch attempt
  -3000 Severe Java 'Error' conditions (OutOfMemoryError, StackOverflowError,
        etc.) during URI processing.
  -4000 'chaff' detection of traps/content of negligible value applied
  -4001 Too many link hops away from seed
  -4002 Too many embed/transitive hops away from last URI in scope
  -5000 Out of scope upon reexamination (only happens if scope changes during 
        crawl)
  -5001 Blocked from fetch by user setting
  -5002 Blocked by a custom processor
  -5003 Blocked due to exceeding an established quota
  -5004 Blocked due to exceeding an established runtime
  -6000 Deleted from Frontier by user
  -7000 Processing thread was killed by the operator (perhaps because of a
        hung condition)
  -9998 Robots.txt rules precluded fetch</programlisting></para>

          <note>
            <para>Codes and explainations are also available under the Help
            link in the web UI.</para>
          </note>

          <para>Please note that status codes defined by Heritrix may be
          subject to change between versions, especially new codes may be
          added to tackle a wider array of situations.</para>
        </glossdef>
      </glossentry>

      <glossentry id="surt">
        <glossterm>SURT</glossterm>

        <glossdef>
          <para>SURT stands for Sort-friendly URI Reordering Transform, and is
          a transformation applied to URIs which makes their left-to-right
          representation better match the natural hierarchy of domain
          names.</para>

          <para>A URI &lt;scheme://domain.tld/path?query&gt; has SURT form
          &lt;scheme://(tld,domain,)/path?query&gt;.</para>

          <para>Conversion to SURT form also involves making all characters
          lowercase, and changing the 'https' scheme to 'http'. Further, the
          '/' after a URI authority component -- for example, the third slash
          in a regular HTTP URI -- will only appear in the SURT form if it
          appeared in the plain URI form. (This convention proves important
          when using real URIs as a shorthand for SURT prefixes, as described
          below.)</para>

          <para>SURT form URIs are typically not used to specify exact URIs
          for fetching. Rather, SURT form is useful when comparing or sorting
          URIs. For example, URIs in SURT format sort into natural groups --
          all 'archive.org' URIs will be adjacent, regardless of what
          subdomains like 'books.archive.org' or 'movies.archive.org' are
          used.</para>

          <para>Most importantly, a SURT form URI, or a truncated version of a
          SURT form URI, can be used as a <xref linkend="surtprefix" />. A
          SURT prefix will often correspond to all URIs within a common 'area'
          of interest for crawling. For example, the prefix
          &lt;http://(is,&gt; will be shared by all URIs in the '.is'
          top-level domain.</para>
        </glossdef>
      </glossentry>

      <glossentry id="surtprefix">
        <glossterm>SURT prefix</glossterm>

        <glossdef>
          <para>A URI in <xref linkend="surt" /> form, especially if
          truncated, may be of use as a "SURT prefix", a shared prefix string
          of all SURT form URIs in the same 'area' of interest for web
          crawling.</para>

          <para>For example, the prefix &lt;http://(is,&gt; will be shared by
          all SURT form URIs in the '.is' top-level domain. The prefix
          &lt;http://(org,archive,www,)/movies&gt; (which is also a valid full
          SURT form URI) will be shared by all URIs at www.archive.org with a
          path beginning '/movies'.</para>

          <para>A collection of sorted SURT prefixes is an efficient way to
          specify a desired crawl scope: any URI whose SURT form starts with
          any of the prefixes should be included.</para>

          <para>A small set of conventions can be also be used to calculate an
          "implied SURT prefix" from a regular URI, such as a URI supplied as
          a crawl seed. These conventions are:</para>

          <orderedlist>
            <listitem>
              <para>Convert the URI to its SURT form.</para>
            </listitem>

            <listitem>
              <para>If there are at least 3 slashes ('/') in the SURT form,
              remove everything after the last slash. As examples,
              &lt;http://(org,example,www,)/main/subsection/&gt; is unchanged;
              &lt;http://(org,example,www,)/main/subsection&gt; is truncated
              to &lt;http://(org,example,www,)/main/&gt;;
              &lt;http://(org,example,www,)/&gt; is unchanged; and
              &lt;http://(org,example,www,)&gt; is unchanged.</para>
            </listitem>

            <listitem>
              <para>If the resulting form ends in an off-parenthesis (')'),
              remove the off-parenthesis. So each of the above examples except
              for the last is unchanged, while the last
              &lt;http://(org,example,www,)&gt; becomes
              &lt;http://(org,example,www,&gt;.</para>
            </listitem>
          </orderedlist>

          <para>This allows many seed URIs, in their usual form, to imply the
          most useful SURT prefixes for crawling related URIs -- with the
          presence or absence of a trailing '/' on URIs without further
          path-info being a subtle indicator as to whether subdomains of the
          supplied domain should be included.</para>

          <para>For example, seed &lt;http://www.archive.org/&gt; will become
          SURT form and implied SURT prefix
          &lt;http://(org,archive,www,)/&gt;, and is the prefix of all SURT
          form URIs on www.archive.org. However, any subdomain URI like
          &lt;http://homepages.www.archive.org/directory&gt; would be ruled
          out, because its SURT form
          &lt;http://(org,archive,www,homepages,)/directory&gt; does not begin
          with the full SURT prefix, including the ')/', deduced from the
          seed.</para>

          <para>In contrast, seed &lt;http://www.archive.org&gt; (note the
          lack of trailing slash) will become SURT form
          &lt;http://(org,archive,www,)&gt;, and implied SURT prefix
          &lt;http://(org,archive,www,&gt; (note the lack of trailing ')').
          This will be the prefix of all URIs on www.archive.org, as well as
          any subdomain URIs like
          &lt;http://homepages.www.archive.org/directory&gt;, because the full
          SURT prefix appears in subdomain URI SURT forms.</para>
        </glossdef>
      </glossentry>

      <glossentry id="toethreads">
        <glossterm>Toe Threads</glossterm>

        <glossdef>
          <para>When crawling Heritrix employs a configurable number of Toe
          Threads to process each URI.</para>

          <para>Each of these threads will request a URI from the Frontier
          (<xref linkend="frontier" />), apply each of the set Processors
          (<xref linkend="processors" />) to it and finally report it as
          completed back to the Frontier.</para>
        </glossdef>
      </glossentry>
    </glossdiv>
  </glossary>
</article>
